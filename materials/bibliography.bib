@article{allenBacterialGrowthStatistical2019,
  title = {Bacterial Growth: A Statistical Physicist's Guide},
  shorttitle = {Bacterial Growth},
  author = {Allen, Rosalind J and Waclaw, Bart{\l}omiej},
  year = {2019},
  month = jan,
  journal = {Reports on progress in physics. Physical Society (Great Britain)},
  volume = {82},
  number = {1},
  pages = {016601},
  issn = {0034-4885},
  doi = {10.1088/1361-6633/aae546},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330087/},
  urldate = {2021-12-13},
  abstract = {Bacterial growth presents many beautiful phenomena that pose new theoretical challenges to statistical physicists, and are also amenable to laboratory experimentation. This review provides some of the essential biological background, discusses recent applications of statistical physics in this field, and highlights the potential for future research.},
  pmcid = {PMC6330087},
  pmid = {30270850},
  file = {/Users/tedgro/Zotero/storage/R45MNWE3/Allen and Waclaw - 2019 - Bacterial growth a statistical physicistâ€™s guide.pdf}
}

@article{vehtariRankNormalizationFoldingLocalization2021,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R\textasciicircum}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
  urldate = {2022-01-03},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum{} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum{} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/tedgro/Zotero/storage/FPRAEFHZ/Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf;/Users/tedgro/Zotero/storage/ST53UIWZ/20-BA1221.html}
}

@misc{betancourtDiagnosingBiasedInference2017,
  title = {Diagnosing {{Biased Inference}} with {{Divergences}}},
  author = {Betancourt, Michael},
  year = {2017},
  journal = {betanalpha.github.io},
  url = {https://github.com/betanalpha/knitr_case_studies/tree/master/divergences_and_bias},
  urldate = {2023-02-13},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/ZYV3R7VK/writing.html}
}

@article{vehtariSurveyBayesianPredictive2012,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  year = {2012},
  month = jan,
  journal = {Statistics Surveys},
  volume = {6},
  number = {none},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  url = {https://projecteuclid.org/journals/statistics-surveys/volume-6/issue-none/A-survey-of-Bayesian-predictive-methods-for-model-assessment-selection/10.1214/12-SS102.full},
  urldate = {2022-05-25},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  keywords = {62-02,62C10,Bayesian,cross-validation,decision theory,Expected utility,information criteria,model assessment,Model selection,predictive},
  file = {/Users/tedgro/Zotero/storage/B6SLH4ZC/Vehtari and Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf;/Users/tedgro/Zotero/storage/I53LDYFT/12-SS102.html}
}

@article{landesObjectiveBayesianismMaximum2013,
  title = {Objective {{Bayesianism}} and the {{Maximum Entropy Principle}}},
  author = {Landes, J{\"u}rgen and Williamson, Jon},
  year = {2013},
  month = sep,
  journal = {Entropy},
  volume = {15},
  number = {12},
  pages = {3528--3591},
  issn = {1099-4300},
  doi = {10.3390/e15093528},
  url = {http://www.mdpi.com/1099-4300/15/9/3528},
  urldate = {2023-02-13},
  abstract = {Objective Bayesian epistemology invokes three norms: the strengths of our beliefs should be probabilities, they should be calibrated to our evidence of physical probabilities, and they should otherwise equivocate sufficiently between the basic propositions that we can express. The three norms are sometimes explicated by appealing to the maximum entropy principle, which says that a belief function should be a probability function, from all those that are calibrated to evidence, that has maximum entropy. However, the three norms of objective Bayesianism are usually justified in different ways. In this paper we show that the three norms can all be subsumed under a single justification in terms of minimising worst-case expected loss. This, in turn, is equivalent to maximising a generalised notion of entropy. We suggest that requiring language invariance, in addition to minimising worst-case expected loss, motivates maximisation of standard entropy as opposed to maximisation of other instances of generalised entropy.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/CSGJNP2C/Landes and Williamson - 2013 - Objective Bayesianism and the Maximum Entropy Prin.pdf}
}

@article{vehtariPracticalBayesianModel2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  month = sep,
  journal = {Statistics and Computing},
  volume = {27},
  number = {5},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  pages = {1413--1432},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://arxiv.org/abs/1507.04544},
  urldate = {2021-02-23},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/554NNNS2/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf}
}

@article{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2020-11-05},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/XEI2Q7F4/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/tedgro/Zotero/storage/8Y5YAYXE/2011.html}
}

@misc{boxBayesianInferenceStatistical1992,
  title = {Bayesian Inference in Statistical Analysis},
  author = {Box, George E. P. and Tiao, George C.},
  year = {1992},
  series = {A {{Wiley-Interscience}} Publication},
  edition = {Wiley classics library ed},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-57428-6},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/7GGTD6UY/Box and Tiao - 1992 - Bayesian inference in statistical analysis.pdf},
  url = {https://onlinelibrary-wiley-com.proxy.findit.cvt.dk/doi/epdf/10.1002/9781118033197}
}

@misc{jaynesProbabilityTheoryLogic2003,
  title = {Probability Theory: The Logic of Science},
  shorttitle = {Probability Theory},
  author = {Jaynes, E. T.},
  editor = {Bretthorst, G. Larry},
  year = {2003},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK}},
  abstract = {A comprehensive introduction to the role of probability theory in general scientific endeavour. This book provides an original interpretation of probability theory, showing the subject to be an extension of logic, and presenting new results and applications. Ideal for scientists working in any area involving inference from incomplete information},
  isbn = {978-0-511-06589-7},
  langid = {english},
  annotation = {OCLC: 57254076},
  file = {/Users/tedgro/Zotero/storage/GYSCLGZQ/Jaynes - 2003 - Probability theory the logic of science.pdf},
  howpublished = "\url{https://readyforai.com/download/probability-theory-the-logic-of-science-pdf/}"
}

@article{laplaceMemoirProbabilityCauses1986,
  title = {Memoir on the {{Probability}} of the {{Causes}} of {{Events}}},
  author = {Laplace, Pierre Simon},
  year = {1986},
  month = aug,
  journal = {Statistical Science},
  volume = {1},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/ss/1177013621},
  url = {https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Memoir-on-the-Probability-of-the-Causes-of-Events/10.1214/ss/1177013621.full},
  urldate = {2024-01-24},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/Y4F5EASP/Laplace - 1986 - Memoir on the Probability of the Causes of Events.pdf}
}

@article{stiglerLaplace1774Memoir1986,
  title = {Laplace's 1774 {{Memoir}} on {{Inverse Probability}}},
  author = {Stigler, Stephen M.},
  year = {1986},
  month = aug,
  journal = {Statistical Science},
  volume = {1},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/ss/1177013620},
  url = {https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Laplaces-1774-Memoir-on-Inverse-Probability/10.1214/ss/1177013620.full},
  urldate = {2024-01-24},
  abstract = {Laplace's first major article on mathematical statistics was published in 1774. It is arguably the most influential article in this field to appear before 1800, being the first widely read presentation of inverse probability and its application to both binomial and location parameter estimation. After a brief introduction, an English translation of this epochal memoir is given.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/HJDVPHIE/Stigler - 1986 - Laplace's 1774 Memoir on Inverse Probability.pdf}
}

@misc{HierarchicalModeling,
  title = {Hierarchical {{Modeling}}},
  author = {Betancourt, Michael},
  year = {2024},
  url = {https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html},
  urldate = {2024-01-31},
  file = {/Users/tedgro/Zotero/storage/MTHXTZMK/hierarchical_modeling.html}
}

@misc{timonenImportanceSamplingApproach2022,
  title = {An Importance Sampling Approach for Reliable and Efficient Inference in {{Bayesian}} Ordinary Differential Equation Models},
  author = {Timonen, Juho and Siccha, Nikolas and Bales, Ben and L{\"a}hdesm{\"a}ki, Harri and Vehtari, Aki},
  year = {2022},
  month = may,
  number = {arXiv:2205.09059},
  eprint = {2205.09059},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09059},
  url = {http://arxiv.org/abs/2205.09059},
  urldate = {2023-10-20},
  abstract = {Statistical models can involve implicitly defined quantities, such as solutions to nonlinear ordinary differential equations (ODEs), that unavoidably need to be numerically approximated in order to evaluate the model. The approximation error inherently biases statistical inference results, but the amount of this bias is generally unknown and often ignored in Bayesian parameter inference. We propose a computationally efficient method for verifying the reliability of posterior inference for such models, when the inference is performed using Markov chain Monte Carlo methods. We validate the efficiency and reliability of our workflow in experiments using simulated and real data, and different ODE solvers. We highlight problems that arise with commonly used adaptive ODE solvers, and propose robust and effective alternatives which, accompanied by our workflow, can now be taken into use without losing reliability of the inferences.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation},
  file = {/Users/tedgro/Zotero/storage/CS49WU93/Timonen et al. - 2022 - An importance sampling approach for reliable and e.pdf;/Users/tedgro/Zotero/storage/H8WUCLLE/2205.html}
}

@article{betancourtConceptualIntroductionHamiltonian2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2020-09-28},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/F674T82K/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf}
}

@article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.1699114},
  url = {https://pubs.aip.org/jcp/article/21/6/1087/202680/Equation-of-State-Calculations-by-Fast-Computing},
  urldate = {2024-02-19},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/TGEGFF59/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf}
}

@article{strumbeljPresentFutureSoftware,
  title = {Past, {{Present}}, and {{Future}} of {{Software}} for {{Bayesian Inference}}},
  year = {2023},
  author = {{\v S}trumbelj, Erik and {Bouchard-C{\^o}t{\'e}}, Alexandre and Corander, Jukka and Gelman, Andrew and Rue, H{\aa}vard and Murray, Lawrence and Pesonen, Henri and Plummer, Martyn and Vehtari, Aki},
  abstract = {Software tools for Bayesian inference have undergone rapid evolution in the past three decades, following popularisation of the first generation MCMC-sampler implementations. More recently, exponential growth in the number of users has been stimulated both by the active development of new packages by the machine learning community and popularity of specialist software for particular applications. This review aims to summarize the most popular software and provide a useful map for a reader to navigate the world of Bayesian computation. We anticipate a vigorous continued development of algorithms and corresponding software in multiple research fields, such as probabilistic programming, likelihood-free inference, and Bayesian neural networks, which will further broaden the possibilities for employing the Bayesian paradigm in exciting applications. Key words and phrases: statistics, data analysis, MCMC, computation, probabilistic programming.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/F55H2HPD/Å trumbelj et al. - Past, Present, and Future of Software for Bayesian.pdf}
}

@misc{beskosOptimalTuningHybrid2010,
  title = {Optimal Tuning of the {{Hybrid Monte-Carlo Algorithm}}},
  author = {Beskos, Alexandros and Pillai, Natesh S. and Roberts, Gareth O. and {Sanz-Serna}, Jesus M. and Stuart, Andrew M.},
  year = {2010},
  month = jan,
  number = {arXiv:1001.4460},
  eprint = {1001.4460},
  primaryclass = {math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1001.4460},
  url = {http://arxiv.org/abs/1001.4460},
  urldate = {2024-02-19},
  abstract = {We investigate the properties of the Hybrid Monte-Carlo algorithm (HMC) in high dimensions. HMC develops a Markov chain reversible w.r.t. a given target distribution \${\textbackslash}Pi\$ by using separable Hamiltonian dynamics with potential \$-{\textbackslash}log{\textbackslash}Pi\$. The additional momentum variables are chosen at random from the Boltzmann distribution and the continuous-time Hamiltonian dynamics are then discretised using the leapfrog scheme. The induced bias is removed via a Metropolis-Hastings accept/reject rule. In the simplified scenario of independent, identically distributed components, we prove that, to obtain an \${\textbackslash}mathcal\{O\}(1)\$ acceptance probability as the dimension \$d\$ of the state space tends to \${\textbackslash}infty\$, the leapfrog step-size \$h\$ should be scaled as \$h= l {\textbackslash}times d\^\{-1/4\}\$. Therefore, in high dimensions, HMC requires \${\textbackslash}mathcal\{O\}(d\^\{1/4\})\$ steps to traverse the state space. We also identify analytically the asymptotically optimal acceptance probability, which turns out to be 0.651 (to three decimal places). This is the choice which optimally balances the cost of generating a proposal, which \{{\textbackslash}em decreases\} as \$l\$ increases, against the cost related to the average number of proposals required to obtain acceptance, which \{{\textbackslash}em increases\} as \$l\$ increases.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/tedgro/Zotero/storage/YXSA5EVQ/Beskos et al. - 2010 - Optimal tuning of the Hybrid Monte-Carlo Algorithm.pdf;/Users/tedgro/Zotero/storage/LASIC2EJ/1001.html}
}

@misc{betancourtConvergenceMarkovChain2018,
  title = {The {{Convergence}} of {{Markov}} Chain {{Monte Carlo Methods}}: {{From}} the {{Metropolis}} Method to {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{Convergence}} of {{Markov}} Chain {{Monte Carlo Methods}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jan,
  number = {arXiv:1706.01520},
  eprint = {1706.01520},
  primaryclass = {physics, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.01520},
  url = {http://arxiv.org/abs/1706.01520},
  urldate = {2024-02-19},
  abstract = {From its inception in the 1950s to the modern frontiers of applied statistics, Markov chain Monte Carlo has been one of the most ubiquitous and successful methods in statistical computing. In that time its development has been fueled by increasingly difficult problems and novel techniques from physics. In this article I will review the history of Markov chain Monte Carlo from its inception with the Metropolis method to today's state-of-the-art in Hamiltonian Monte Carlo. Along the way I will focus on the evolving interplay between the statistical and physical perspectives of the method.},
  archiveprefix = {arxiv},
  keywords = {Physics - History and Philosophy of Physics,Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/YS3RE63A/Betancourt - 2018 - The Convergence of Markov chain Monte Carlo Method.pdf;/Users/tedgro/Zotero/storage/HLEWR8BI/1706.html}
}

@article{andrieuIntroductionMCMCMachine,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  year = {2003},
  author = {Andrieu, Christophe and Andrieu, C},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/VIZS8UWZ/Andrieu and Andrieu - An Introduction to MCMC for Machine Learning.pdf}
}

