@article{allenBacterialGrowthStatistical2019,
  title = {Bacterial Growth: A Statistical Physicist's Guide},
  shorttitle = {Bacterial Growth},
  author = {Allen, Rosalind J and Waclaw, Bart{\l}omiej},
  year = {2019},
  month = jan,
  journal = {Reports on progress in physics. Physical Society (Great Britain)},
  volume = {82},
  number = {1},
  pages = {016601},
  issn = {0034-4885},
  doi = {10.1088/1361-6633/aae546},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330087/},
  urldate = {2021-12-13},
  abstract = {Bacterial growth presents many beautiful phenomena that pose new theoretical challenges to statistical physicists, and are also amenable to laboratory experimentation. This review provides some of the essential biological background, discusses recent applications of statistical physics in this field, and highlights the potential for future research.},
  pmcid = {PMC6330087},
  pmid = {30270850},
  file = {/Users/tedgro/Zotero/storage/R45MNWE3/Allen and Waclaw - 2019 - Bacterial growth a statistical physicistâ€™s guide.pdf}
}

@article{vehtariRankNormalizationFoldingLocalization2021,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R\textasciicircum}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
  urldate = {2022-01-03},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum{} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum{} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/tedgro/Zotero/storage/FPRAEFHZ/Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf;/Users/tedgro/Zotero/storage/ST53UIWZ/20-BA1221.html}
}

@misc{betancourtDiagnosingBiasedInference2017,
  title = {Diagnosing {{Biased Inference}} with {{Divergences}}},
  author = {Betancourt, Michael},
  year = {2017},
  journal = {betanalpha.github.io},
  url = {https://github.com/betanalpha/knitr_case_studies/tree/master/divergences_and_bias},
  urldate = {2023-02-13},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/ZYV3R7VK/writing.html}
}

@article{vehtariSurveyBayesianPredictive2012,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  year = {2012},
  month = jan,
  journal = {Statistics Surveys},
  volume = {6},
  number = {none},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  url = {https://projecteuclid.org/journals/statistics-surveys/volume-6/issue-none/A-survey-of-Bayesian-predictive-methods-for-model-assessment-selection/10.1214/12-SS102.full},
  urldate = {2022-05-25},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  keywords = {62-02,62C10,Bayesian,cross-validation,decision theory,Expected utility,information criteria,model assessment,Model selection,predictive},
  file = {/Users/tedgro/Zotero/storage/B6SLH4ZC/Vehtari and Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf;/Users/tedgro/Zotero/storage/I53LDYFT/12-SS102.html}
}

@article{landesObjectiveBayesianismMaximum2013,
  title = {Objective {{Bayesianism}} and the {{Maximum Entropy Principle}}},
  author = {Landes, J{\"u}rgen and Williamson, Jon},
  year = {2013},
  month = sep,
  journal = {Entropy},
  volume = {15},
  number = {12},
  pages = {3528--3591},
  issn = {1099-4300},
  doi = {10.3390/e15093528},
  url = {http://www.mdpi.com/1099-4300/15/9/3528},
  urldate = {2023-02-13},
  abstract = {Objective Bayesian epistemology invokes three norms: the strengths of our beliefs should be probabilities, they should be calibrated to our evidence of physical probabilities, and they should otherwise equivocate sufficiently between the basic propositions that we can express. The three norms are sometimes explicated by appealing to the maximum entropy principle, which says that a belief function should be a probability function, from all those that are calibrated to evidence, that has maximum entropy. However, the three norms of objective Bayesianism are usually justified in different ways. In this paper we show that the three norms can all be subsumed under a single justification in terms of minimising worst-case expected loss. This, in turn, is equivalent to maximising a generalised notion of entropy. We suggest that requiring language invariance, in addition to minimising worst-case expected loss, motivates maximisation of standard entropy as opposed to maximisation of other instances of generalised entropy.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/CSGJNP2C/Landes and Williamson - 2013 - Objective Bayesianism and the Maximum Entropy Prin.pdf}
}

@article{vehtariPracticalBayesianModel2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  month = sep,
  journal = {Statistics and Computing},
  volume = {27},
  number = {5},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  pages = {1413--1432},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://arxiv.org/abs/1507.04544},
  urldate = {2021-02-23},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/554NNNS2/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf}
}

@article{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2020-11-05},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/tedgro/Zotero/storage/XEI2Q7F4/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/tedgro/Zotero/storage/8Y5YAYXE/2011.html}
}

@book{boxBayesianInferenceStatistical1992,
  title = {Bayesian Inference in Statistical Analysis},
  author = {Box, George E. P. and Tiao, George C.},
  year = {1992},
  series = {A {{Wiley-Interscience}} Publication},
  edition = {Wiley classics library ed},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-57428-6},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/7GGTD6UY/Box and Tiao - 1992 - Bayesian inference in statistical analysis.pdf}
}
