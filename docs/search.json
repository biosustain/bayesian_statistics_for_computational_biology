[
  {
    "objectID": "hierarchical_models.html",
    "href": "hierarchical_models.html",
    "title": "Hierarchical models",
    "section": "",
    "text": "In general: a model with hyperparameters, i.e. parameters that probabilistically control other parameters.\nE.g.\n\\[\\begin{align*}\ny_i &\\sim N(\\alpha_{group(i)} + \\beta \\cdot x_i, \\sigma) \\\\\n\\alpha{group(i)} &\\sim N(\\mu, \\tau)\n\\end{align*}\\]\nIn this model \\(\\tau\\) is a hyperparameter. Is \\(\\mu\\) a hyperparameter???\nHierarchical models are great for describing the situation where you know some measurements have something in common (e.g. they come from the same group), but you don’t know how much.\nLearn more!",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "hierarchical_models.html#theory-what-is-a-hierarchical-model",
    "href": "hierarchical_models.html#theory-what-is-a-hierarchical-model",
    "title": "Hierarchical models",
    "section": "",
    "text": "In general: a model with hyperparameters, i.e. parameters that probabilistically control other parameters.\nE.g.\n\\[\\begin{align*}\ny_i &\\sim N(\\alpha_{group(i)} + \\beta \\cdot x_i, \\sigma) \\\\\n\\alpha{group(i)} &\\sim N(\\mu, \\tau)\n\\end{align*}\\]\nIn this model \\(\\tau\\) is a hyperparameter. Is \\(\\mu\\) a hyperparameter???\nHierarchical models are great for describing the situation where you know some measurements have something in common (e.g. they come from the same group), but you don’t know how much.\nLearn more!",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "hierarchical_models.html#example-always-be-closing",
    "href": "hierarchical_models.html#example-always-be-closing",
    "title": "Hierarchical models",
    "section": "Example: always be closing!",
    "text": "Example: always be closing!\nPlushycorp employs 10 salespeople who go door to door selling cute plushies. The number of plushies that each salesperson sold every working day for two weeks was recorded. What can Plushycorp find out from this data?\nTo answer the question in a best-case scenario, we can use a hierarchical model to run a “digital twin” of this experiment with known parameters and data generating process. Specifically, we can assume that the number \\(y_{ij}\\) of plushies that salesperson \\(i\\) sells on day \\(j\\) depends on a combination of factors:\n\nThe baseline amount \\(\\mu\\) that a totally average salesperson would sell on a normal day\nThe salesperson’s ability \\(ability_i\\)\nAn effect \\(day\\ effect_j\\) for the day of the week: people are thought to buy fewer and fewer plushies as the week drags on.\nSome random variation\n\nA good first step for modelling count data is the Poisson distribution, so let’s assume that the sales measurements follow the following Poisson distribution:1\n1 Note the use of the log link function.\\[\\begin{align*}\ny_{ij} &\\sim Poisson(\\lambda) \\\\\n\\ln\\lambda &= \\mu + ability_i + day\\ effect_j\n\\end{align*}\\]\nWe know that the salespeople have different abilities, but how just different are they? Since this isn’t really clear to Plushycorp, it makes sense to introduce a parameter \\(\\tau_{ability}\\) into the model:\n\\[\\begin{equation*}\nability \\sim N(0, \\tau^{ability})\n\\end{equation*}\\]\nNow we have a hierarchical model!\nWe can make a similar argument for the day of the week effects:2\n2 Can you think of a better model for day effects given the information above??\\[\\begin{equation*}\nday\\ effect \\sim N(0, \\tau^{day})\n\\end{equation*}\\]\nFinally we can complete our model by specifying prior distributions for the non-hierarchical parameters:3\n3 \\(HN\\) here refers to the “half-normal” distribution, a decent default prior for hierarchical standard deviations\\[\\begin{align*}\n\\mu &\\sim LN(0, 1) \\\\\n\\tau_ability &\\sim HN(0, 1) \\\\\n\\tau_day &\\sim HN(0, 1)\n\\end{align*}\\]\nTo test out our model with fake data, we can use Python to generate a fake set of salespeople and days, then generate some sales consistently with our model. Next we can generate some data,\n\nfrom pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\n\nN_SALESPERSON = 10\nN_WEEK = 2\nDAY_NAMES = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\nBASELINE = 2  # 2 plushies in one day is fine\nTAU_ABILITY = 0.35\nTAU_DAY = 0.2\n\nSEED = 12345\nDATA_DIR = Path(\"../data\")\n\nrng = np.random.default_rng(seed=SEED)\n\nwith open(DATA_DIR / \"names.json\", \"r\") as f:\n    name_directory = json.load(f)\n\nnames = [\n    f\"{first_name} {surname}\"\n    for first_name, surname in zip(\n        *map(\n            lambda l: rng.choice(l, size=N_SALESPERSON, replace=False),\n            name_directory.values()\n        )\n    )\n]\n\nabilities = rng.normal(loc=0, scale=TAU_ABILITY, size=N_SALESPERSON)\n\nsalespeople = pd.DataFrame({\"salesperson\": names, \"ability\": abilities})\n\nsalespeople\n\n\n\n\n\n\n\n\n\nsalesperson\nability\n\n\n\n\n0\nMorten Andersen\n0.489643\n\n\n1\nLene Poulsen\n0.462804\n\n\n2\nRasmus Jensen\n-0.104894\n\n\n3\nHanne Madsen\n0.316022\n\n\n4\nMette Rasmussen\n-0.567554\n\n\n5\nChristian Christensen\n-0.055366\n\n\n6\nHelle Kristensen\n0.157319\n\n\n7\nCharlotte Hansen\n-0.470260\n\n\n8\nMaria Petersen\n-0.028591\n\n\n9\nJette Thomsen\n0.603659\n\n\n\n\n\n\n\n\n\nday_effects = sorted(\n    rng.normal(loc=0, scale=TAU_DAY, size=len(DAY_NAMES))\n)[::-1]  # This (i.e. `[::-1]`) is a nice way to reverse a list\ndays = pd.DataFrame({\"day\": DAY_NAMES, \"day_effect\": day_effects})\ndays\n\n\n\n\n\n\n\n\n\nday\nday_effect\n\n\n\n\n0\nMon\n0.523632\n\n\n1\nTue\n0.165727\n\n\n2\nWed\n0.155472\n\n\n3\nThu\n-0.191798\n\n\n4\nFri\n-0.241878\n\n\n\n\n\n\n\n\n\nsales = (\n    days\n    .merge(salespeople, how=\"cross\")\n    .merge(pd.DataFrame({\"week\":[1, 2, 3, 4]}), how=\"cross\")\n    .assign(\n        sales=lambda df: rng.poisson(\n            np.exp(np.log(BASELINE) + df[\"ability\"] + df[\"day_effect\"])\n        )\n    )\n    [[\"week\", \"day\", \"salesperson\", \"day_effect\", \"ability\", \"sales\"]]\n    .copy()\n)\nsales.head()\n\n\n\n\n\n\n\n\n\nweek\nday\nsalesperson\nday_effect\nability\nsales\n\n\n\n\n0\n1\nMon\nMorten Andersen\n0.523632\n0.489643\n10\n\n\n1\n2\nMon\nMorten Andersen\n0.523632\n0.489643\n3\n\n\n2\n3\nMon\nMorten Andersen\n0.523632\n0.489643\n4\n\n\n3\n4\nMon\nMorten Andersen\n0.523632\n0.489643\n4\n\n\n4\n1\nMon\nLene Poulsen\n0.523632\n0.462804\n4\n\n\n\n\n\n\n\n\nHere is the fortnightly sales chart\n\ntotal_sales = (\n    sales.groupby(\"salesperson\")[\"sales\"].sum().sort_values(ascending=False)\n)\n\ntotal_sales.plot(kind=\"bar\", ylabel=\"Plushies sold\", title=\"Fortnightly sales\")\n\n\n\n\n\n\n\n\nIt’s pretty straightforward to represent hierarchical models with Stan, almost like Stan was designed for it!\n\nfrom cmdstanpy import CmdStanModel\n\nmodel = CmdStanModel(stan_file=\"../src/stan/plushies.stan\")\nprint(model.code())\n\ndata {\n int&lt;lower=1&gt; N;\n int&lt;lower=1&gt; N_salesperson;\n int&lt;lower=1&gt; N_day;\n array[N] int&lt;lower=1,upper=N_salesperson&gt; salesperson;\n array[N] int&lt;lower=1,upper=N_day&gt; day;\n array[N] int&lt;lower=0&gt; sales;\n int&lt;lower=0,upper=1&gt; likelihood;\n}\nparameters {\n real log_mu;\n vector[N_salesperson] ability;\n vector[N_day] day_effect;\n real&lt;lower=0&gt; tau_ability;\n real&lt;lower=0&gt; tau_day;\n}\ntransformed parameters {\n vector[N] log_lambda = log_mu + ability[salesperson] + day_effect[day]; \n}\nmodel {\n  log_mu ~ normal(0, 1);\n  ability ~ normal(0, tau_ability);\n  day_effect ~ normal(0, tau_day);\n  tau_ability ~ normal(0, 0.5);\n  tau_day ~ normal(0, 0.5);\n  if (likelihood){\n    sales ~ poisson_log(log_lambda);\n  }\n}\ngenerated quantities {\n real mu = exp(log_mu);\n vector[N] lambda = exp(log_lambda);\n array[N] int yrep = poisson_rng(lambda);\n vector[N] llik; \n for (n in 1:N){\n   llik[n] = poisson_lpmf(sales[n] | lambda[n]);\n }\n}\n\n\n\n\n\nimport arviz as az\nfrom stanio.json import process_dictionary\n\ndef one_encode(l):\n    \"\"\"One-encode a 1d list-like thing.\"\"\"\n    return dict(zip(l, range(1, len(l) + 1)))\n\n\nsalesperson_codes = one_encode(salespeople[\"salesperson\"])\nday_codes = one_encode(days[\"day\"])\ndata_prior = process_dictionary({\n        \"N\": len(sales),\n        \"N_salesperson\": len(salespeople),\n        \"N_day\": len(days),\n        \"salesperson\": sales[\"salesperson\"].map(salesperson_codes),\n        \"day\": sales[\"day\"].map(day_codes),\n        \"sales\": sales[\"sales\"],\n        \"likelihood\": 0\n    }\n)\ndata_posterior = data_prior | {\"likelihood\": 1}\nmcmc_prior = model.sample(data=data_prior)\nmcmc_posterior = model.sample(data=data_posterior)\nidata = az.from_cmdstanpy(\n    posterior=mcmc_posterior,\n    prior=mcmc_prior,\n    log_likelihood=\"llik\",\n    posterior_predictive=\"yrep\",\n    observed_data=data_posterior,\n    coords={\n        \"salesperson\": salespeople[\"salesperson\"],\n        \"day\": days[\"day\"],\n        \"observation\": sales.index\n    },\n    dims={\n        \"lambda\": [\"observation\"],\n        \"ability\": [\"salesperson\"],\n        \"day_effect\": [\"day\"],\n        \"llik\": [\"observation\"],\n        \"yrep\": [\"observation\"]\n    }\n)\nidata\n\n17:23:34 - cmdstanpy - INFO - CmdStan start processing\n17:23:35 - cmdstanpy - INFO - CmdStan done processing.\n17:23:35 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'plushies.stan', line 23, column 2 to column 34)\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'plushies.stan', line 22, column 2 to column 35)\nConsider re-running with show_console=True if the above output is unclear!\n17:23:35 - cmdstanpy - WARNING - Some chains may have failed to converge.\n    Chain 1 had 19 divergent transitions (1.9%)\n    Chain 2 had 1 divergent transitions (0.1%)\n    Chain 3 had 17 divergent transitions (1.7%)\n    Chain 4 had 4 divergent transitions (0.4%)\n    Use the \"diagnose()\" method on the CmdStanMCMC object to see further information.\n17:23:35 - cmdstanpy - INFO - CmdStan start processing\n17:23:35 - cmdstanpy - INFO - CmdStan done processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 13MB\nDimensions:           (chain: 4, draw: 1000, salesperson: 10, day: 5,\n                       log_lambda_dim_0: 200, observation: 200)\nCoordinates:\n  * chain             (chain) int64 32B 0 1 2 3\n  * draw              (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * salesperson       (salesperson) object 80B 'Morten Andersen' ... 'Jette T...\n  * day               (day) object 40B 'Mon' 'Tue' 'Wed' 'Thu' 'Fri'\n  * log_lambda_dim_0  (log_lambda_dim_0) int64 2kB 0 1 2 3 4 ... 196 197 198 199\n  * observation       (observation) int64 2kB 0 1 2 3 4 ... 195 196 197 198 199\nData variables:\n    log_mu            (chain, draw) float64 32kB 0.3217 0.7637 ... 0.7574 0.5949\n    ability           (chain, draw, salesperson) float64 320kB 0.6564 ... 0.7226\n    day_effect        (chain, draw, day) float64 160kB 0.5388 0.2417 ... 0.01487\n    tau_ability       (chain, draw) float64 32kB 0.2651 0.6149 ... 0.5042 0.2755\n    tau_day           (chain, draw) float64 32kB 0.7837 1.093 ... 0.315 0.3572\n    log_lambda        (chain, draw, log_lambda_dim_0) float64 6MB 1.517 ... 1...\n    mu                (chain, draw) float64 32kB 1.379 2.146 ... 2.133 1.813\n    lambda            (chain, draw, observation) float64 6MB 4.558 ... 3.79\nAttributes:\n    created_at:                 2024-04-24T15:23:36.080006\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000salesperson: 10day: 5log_lambda_dim_0: 200observation: 200Coordinates: (6)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])salesperson(salesperson)object'Morten Andersen' ... 'Jette Tho...array(['Morten Andersen', 'Lene Poulsen', 'Rasmus Jensen', 'Hanne Madsen',\n       'Mette Rasmussen', 'Christian Christensen', 'Helle Kristensen',\n       'Charlotte Hansen', 'Maria Petersen', 'Jette Thomsen'], dtype=object)day(day)object'Mon' 'Tue' 'Wed' 'Thu' 'Fri'array(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], dtype=object)log_lambda_dim_0(log_lambda_dim_0)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])observation(observation)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])Data variables: (8)log_mu(chain, draw)float640.3217 0.7637 ... 0.7574 0.5949array([[0.321698, 0.763671, 0.335676, ..., 0.914372, 0.739441, 0.793362],\n       [0.733995, 0.831989, 0.753088, ..., 0.635953, 0.372713, 0.710834],\n       [0.913258, 0.633489, 0.635184, ..., 0.638623, 0.613409, 0.557084],\n       [1.11624 , 0.912497, 1.00232 , ..., 0.682813, 0.757361, 0.594949]])ability(chain, draw, salesperson)float640.6564 0.3556 ... -0.3805 0.7226array([[[ 0.656424  ,  0.355597  ,  0.038209  , ...,  0.0551495 ,\n         -0.240986  ,  0.365692  ],\n        [ 0.275656  ,  0.18141   , -0.499279  , ..., -0.530698  ,\n         -0.818105  ,  0.115984  ],\n        [ 0.868173  ,  0.634734  , -0.0787711 , ..., -0.396574  ,\n         -0.138275  ,  0.70951   ],\n        ...,\n        [ 0.56924   ,  0.616181  , -0.209008  , ..., -0.485294  ,\n          0.0629966 ,  0.505582  ],\n        [ 0.234983  ,  0.140909  , -0.0742631 , ..., -0.0931603 ,\n         -0.439399  ,  0.336308  ],\n        [ 0.627241  ,  0.61031   , -0.159887  , ..., -0.231502  ,\n          0.173027  ,  0.483141  ]],\n\n       [[ 0.524738  ,  0.299384  , -0.128399  , ..., -0.321533  ,\n         -0.471469  ,  0.381388  ],\n        [ 0.268094  ,  0.372575  , -0.116848  , ..., -0.167663  ,\n         -0.0939631 ,  0.432046  ],\n        [ 0.416574  ,  0.312579  , -0.0442097 , ..., -0.131695  ,\n         -0.257977  ,  0.388858  ],\n...\n        [ 0.351948  ,  0.351979  , -0.145252  , ..., -0.301309  ,\n         -0.350063  ,  0.61466   ],\n        [ 0.626034  ,  0.488873  , -0.312707  , ..., -0.162539  ,\n          0.086432  ,  0.256012  ],\n        [ 0.386718  ,  0.105803  ,  0.0285374 , ..., -0.171104  ,\n         -0.238082  ,  0.461586  ]],\n\n       [[ 0.497463  ,  0.431769  , -0.184075  , ..., -0.152927  ,\n         -0.451511  ,  0.215089  ],\n        [ 0.314577  ,  0.272895  , -0.0828201 , ..., -0.219228  ,\n         -0.149343  ,  0.408383  ],\n        [ 0.469042  ,  0.433951  , -0.15328   , ..., -0.297962  ,\n         -0.413098  ,  0.331303  ],\n        ...,\n        [ 0.564996  ,  0.291847  , -0.111083  , ..., -0.255979  ,\n         -0.285392  ,  0.250286  ],\n        [ 0.398171  ,  0.529362  , -0.196197  , ..., -0.353758  ,\n          0.00272371,  0.667729  ],\n        [ 0.349982  ,  0.676854  ,  0.082755  , ..., -0.243719  ,\n         -0.380531  ,  0.722563  ]]])day_effect(chain, draw, day)float640.5388 0.2417 ... -0.2732 0.01487array([[[ 0.538834  ,  0.24172   ,  0.653598  ,  0.106026  ,\n          0.0703708 ],\n        [ 0.752733  ,  0.374026  ,  0.341618  , -0.18609   ,\n          0.0935145 ],\n        [ 0.536339  ,  0.392309  ,  0.306961  , -0.0843966 ,\n          0.0631634 ],\n        ...,\n        [ 0.168223  ,  0.0447242 , -0.219424  , -0.319776  ,\n         -0.326938  ],\n        [ 0.378984  ,  0.0613558 ,  0.29689   , -0.419294  ,\n         -0.174796  ],\n        [ 0.316538  ,  0.176052  ,  0.0091236 , -0.237646  ,\n         -0.180957  ]],\n\n       [[ 0.509748  ,  0.1768    ,  0.367457  , -0.243813  ,\n         -0.101096  ],\n        [ 0.30846   ,  0.204004  ,  0.215793  , -0.20621   ,\n         -0.315268  ],\n        [ 0.511558  ,  0.089853  ,  0.19501   , -0.297502  ,\n         -0.0929425 ],\n...\n        [ 0.402922  ,  0.21769   ,  0.340985  , -0.399339  ,\n          0.171885  ],\n        [ 0.550046  ,  0.337528  ,  0.322649  ,  0.0671237 ,\n         -0.325147  ],\n        [ 0.412696  ,  0.189478  ,  0.228586  , -0.366775  ,\n          0.144992  ]],\n\n       [[ 0.192508  , -0.217356  , -0.201901  , -0.500376  ,\n         -0.495931  ],\n        [ 0.174038  ,  0.094703  ,  0.0940813 , -0.511148  ,\n         -0.34743   ],\n        [ 0.219584  , -0.13778   , -0.0866456 , -0.305965  ,\n         -0.202237  ],\n        ...,\n        [ 0.454831  ,  0.415526  ,  0.139629  , -0.295651  ,\n          0.0342073 ],\n        [ 0.197426  , -0.109294  ,  0.169219  , -0.21059   ,\n         -0.326557  ],\n        [ 0.346377  , -0.00477183,  0.344991  , -0.273172  ,\n          0.0148668 ]]])tau_ability(chain, draw)float640.2651 0.6149 ... 0.5042 0.2755array([[0.26513 , 0.614935, 0.522894, ..., 0.479564, 0.209619, 0.409691],\n       [0.39157 , 0.238225, 0.396244, ..., 0.273648, 0.638984, 0.287213],\n       [0.343677, 0.375923, 0.315617, ..., 0.421827, 0.325904, 0.269188],\n       [0.278558, 0.388531, 0.408379, ..., 0.347371, 0.504237, 0.275481]])tau_day(chain, draw)float640.7837 1.093 ... 0.315 0.3572array([[0.783666, 1.09318 , 0.689128, ..., 0.173108, 0.321478, 0.374284],\n       [0.397747, 0.652572, 0.26738 , ..., 0.353891, 0.355792, 0.358409],\n       [0.25583 , 0.271483, 0.397679, ..., 0.244153, 0.514826, 0.186739],\n       [0.289089, 0.333462, 0.22525 , ..., 0.209686, 0.314959, 0.357228]])log_lambda(chain, draw, log_lambda_dim_0)float641.517 1.517 1.517 ... 1.332 1.332array([[[1.51696 , 1.51696 , 1.51696 , ..., 0.757761, 0.757761,\n         0.757761],\n        [1.79206 , 1.79206 , 1.79206 , ..., 0.973169, 0.973169,\n         0.973169],\n        [1.74019 , 1.74019 , 1.74019 , ..., 1.10835 , 1.10835 ,\n         1.10835 ],\n        ...,\n        [1.65184 , 1.65184 , 1.65184 , ..., 1.09302 , 1.09302 ,\n         1.09302 ],\n        [1.35341 , 1.35341 , 1.35341 , ..., 0.900953, 0.900953,\n         0.900953],\n        [1.73714 , 1.73714 , 1.73714 , ..., 1.09555 , 1.09555 ,\n         1.09555 ]],\n\n       [[1.76848 , 1.76848 , 1.76848 , ..., 1.01429 , 1.01429 ,\n         1.01429 ],\n        [1.40854 , 1.40854 , 1.40854 , ..., 0.948766, 0.948766,\n         0.948766],\n        [1.68122 , 1.68122 , 1.68122 , ..., 1.049   , 1.049   ,\n         1.049   ],\n...\n        [1.39349 , 1.39349 , 1.39349 , ..., 1.42517 , 1.42517 ,\n         1.42517 ],\n        [1.78949 , 1.78949 , 1.78949 , ..., 0.544273, 0.544273,\n         0.544273],\n        [1.3565  , 1.3565  , 1.3565  , ..., 1.16366 , 1.16366 ,\n         1.16366 ]],\n\n       [[1.80621 , 1.80621 , 1.80621 , ..., 0.835397, 0.835397,\n         0.835397],\n        [1.40111 , 1.40111 , 1.40111 , ..., 0.973451, 0.973451,\n         0.973451],\n        [1.69095 , 1.69095 , 1.69095 , ..., 1.13139 , 1.13139 ,\n         1.13139 ],\n        ...,\n        [1.70264 , 1.70264 , 1.70264 , ..., 0.967306, 0.967306,\n         0.967306],\n        [1.35296 , 1.35296 , 1.35296 , ..., 1.09853 , 1.09853 ,\n         1.09853 ],\n        [1.29131 , 1.29131 , 1.29131 , ..., 1.33238 , 1.33238 ,\n         1.33238 ]]])mu(chain, draw)float641.379 2.146 1.399 ... 2.133 1.813array([[1.37947, 2.14614, 1.39889, ..., 2.49521, 2.09476, 2.21082],\n       [2.08339, 2.29788, 2.12355, ..., 1.88882, 1.45167, 2.03569],\n       [2.49243, 1.88417, 1.88737, ..., 1.89387, 1.84672, 1.74557],\n       [3.05335, 2.49053, 2.7246 , ..., 1.97944, 2.13264, 1.81294]])lambda(chain, draw, observation)float644.558 4.558 4.558 ... 3.79 3.79array([[[4.55833, 4.55833, 4.55833, ..., 2.13349, 2.13349, 2.13349],\n        [6.0018 , 6.0018 , 6.0018 , ..., 2.64632, 2.64632, 2.64632],\n        [5.69841, 5.69841, 5.69841, ..., 3.02935, 3.02935, 3.02935],\n        ...,\n        [5.21654, 5.21654, 5.21654, ..., 2.98326, 2.98326, 2.98326],\n        [3.8706 , 3.8706 , 3.8706 , ..., 2.46195, 2.46195, 2.46195],\n        [5.68108, 5.68108, 5.68108, ..., 2.99082, 2.99082, 2.99082]],\n\n       [[5.86194, 5.86194, 5.86194, ..., 2.7574 , 2.7574 , 2.7574 ],\n        [4.08999, 4.08999, 4.08999, ..., 2.58252, 2.58252, 2.58252],\n        [5.3721 , 5.3721 , 5.3721 , ..., 2.8548 , 2.8548 , 2.8548 ],\n        ...,\n        [3.71532, 3.71532, 3.71532, ..., 3.14237, 3.14237, 3.14237],\n        [4.74394, 4.74394, 4.74394, ..., 1.97637, 1.97637, 1.97637],\n        [4.06048, 4.06048, 4.06048, ..., 4.03981, 4.03981, 4.03981]],\n\n       [[4.21434, 4.21434, 4.21434, ..., 3.03145, 3.03145, 3.03145],\n        [6.34211, 6.34211, 6.34211, ..., 2.52283, 2.52283, 2.52283],\n        [4.54141, 4.54141, 4.54141, ..., 2.8735 , 2.8735 , 2.8735 ],\n        ...,\n        [4.0289 , 4.0289 , 4.0289 , ..., 4.15856, 4.15856, 4.15856],\n        [5.98639, 5.98639, 5.98639, ..., 1.72336, 1.72336, 1.72336],\n        [3.88257, 3.88257, 3.88257, ..., 3.20164, 3.20164, 3.20164]],\n\n       [[6.08733, 6.08733, 6.08733, ..., 2.30573, 2.30573, 2.30573],\n        [4.05971, 4.05971, 4.05971, ..., 2.64706, 2.64706, 2.64706],\n        [5.42461, 5.42461, 5.42461, ..., 3.09995, 3.09995, 3.09995],\n        ...,\n        [5.48842, 5.48842, 5.48842, ..., 2.63085, 2.63085, 2.63085],\n        [3.86885, 3.86885, 3.86885, ..., 2.99976, 2.99976, 2.99976],\n        [3.63754, 3.63754, 3.63754, ..., 3.79005, 3.79005, 3.79005]]])Indexes: (6)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))salespersonPandasIndexPandasIndex(Index(['Morten Andersen', 'Lene Poulsen', 'Rasmus Jensen', 'Hanne Madsen',\n       'Mette Rasmussen', 'Christian Christensen', 'Helle Kristensen',\n       'Charlotte Hansen', 'Maria Petersen', 'Jette Thomsen'],\n      dtype='object', name='salesperson'))dayPandasIndexPandasIndex(Index(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], dtype='object', name='day'))log_lambda_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='log_lambda_dim_0', length=200))observationPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='observation', length=200))Attributes: (4)created_at :2024-04-24T15:23:36.080006arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:      (chain: 4, draw: 1000, observation: 200)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * observation  (observation) int64 2kB 0 1 2 3 4 5 ... 194 195 196 197 198 199\nData variables:\n    yrep         (chain, draw, observation) float64 6MB 5.0 8.0 2.0 ... 3.0 4.0\nAttributes:\n    created_at:                 2024-04-24T15:23:36.085663\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000observation: 200Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])observation(observation)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])Data variables: (1)yrep(chain, draw, observation)float645.0 8.0 2.0 1.0 ... 4.0 4.0 3.0 4.0array([[[ 5.,  8.,  2., ...,  2.,  1.,  5.],\n        [ 5.,  4.,  1., ...,  1.,  5.,  2.],\n        [ 5., 11.,  7., ...,  4.,  6.,  4.],\n        ...,\n        [ 4.,  9.,  3., ...,  2.,  0.,  5.],\n        [ 2.,  7.,  5., ...,  3.,  5.,  0.],\n        [11.,  3.,  4., ...,  2.,  4.,  3.]],\n\n       [[ 3.,  3.,  7., ...,  2.,  4.,  7.],\n        [ 2.,  6.,  5., ...,  1.,  3.,  2.],\n        [ 5.,  7.,  3., ...,  2.,  3.,  1.],\n        ...,\n        [ 4.,  2.,  5., ...,  1.,  1.,  5.],\n        [ 3.,  7.,  6., ...,  2.,  3.,  1.],\n        [ 3.,  7., 10., ...,  8.,  3.,  2.]],\n\n       [[ 3.,  2.,  5., ...,  4.,  2.,  5.],\n        [ 6., 10.,  4., ...,  2.,  5.,  3.],\n        [10.,  3.,  6., ...,  3.,  3.,  4.],\n        ...,\n        [ 3.,  0.,  6., ...,  1.,  6.,  6.],\n        [ 6.,  4.,  4., ...,  0.,  0.,  3.],\n        [ 3.,  2.,  2., ...,  2.,  5.,  4.]],\n\n       [[ 5.,  7.,  3., ...,  1.,  1.,  4.],\n        [ 1.,  4.,  4., ...,  5.,  3.,  4.],\n        [ 6.,  4.,  9., ...,  4.,  2.,  2.],\n        ...,\n        [ 6.,  7.,  4., ...,  1.,  4.,  2.],\n        [ 5.,  3.,  4., ...,  2.,  2.,  2.],\n        [ 2.,  4.,  2., ...,  4.,  3.,  4.]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))observationPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='observation', length=200))Attributes: (4)created_at :2024-04-24T15:23:36.085663arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:      (chain: 4, draw: 1000, observation: 200)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * observation  (observation) int64 2kB 0 1 2 3 4 5 ... 194 195 196 197 198 199\nData variables:\n    llik         (chain, draw, observation) float64 6MB -4.493 -1.799 ... -1.818\nAttributes:\n    created_at:                 2024-04-24T15:23:36.417443\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000observation: 200Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])observation(observation)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])Data variables: (1)llik(chain, draw, observation)float64-4.493 -1.799 ... -1.585 -1.818array([[[-4.49318, -1.79922, -1.66856, ..., -1.65197, -1.65197,\n         -1.31112],\n        [-3.18562, -2.41738, -2.01162, ..., -1.51857, -1.51857,\n         -1.39313],\n        [-3.40095, -2.26961, -1.91572, ..., -1.49607, -1.49607,\n         -1.5058 ],\n        ...,\n        [-3.80261, -2.0528 , -1.78726, ..., -1.49597, -1.49597,\n         -1.49037],\n        [-5.44092, -1.60213, -1.63502, ..., -1.55085, -1.55085,\n         -1.35319],\n        [-3.41408, -2.26141, -1.91057, ..., -1.49594, -1.49594,\n         -1.49287]],\n\n       [[-3.28155, -2.34826, -1.96607, ..., -1.5063 , -1.5063 ,\n         -1.42197],\n        [-5.10898, -1.65612, -1.63387, ..., -1.52798, -1.52798,\n         -1.37814],\n        [-3.66432, -2.1202 , -1.82528, ..., -1.49955, -1.49955,\n         -1.44994],\n...\n        [-5.19838, -1.64018, -1.63298, ..., -1.67481, -1.67481,\n         -2.00137],\n        [-3.19592, -2.40968, -2.00649, ..., -1.8823 , -1.8823 ,\n         -1.32796],\n        [-5.42201, -1.60484, -1.63463, ..., -1.50241, -1.50241,\n         -1.56746]],\n\n       [[-3.12964, -2.46046, -2.04055, ..., -1.5913 , -1.5913 ,\n         -1.32808],\n        [-5.153  , -1.64814, -1.63332, ..., -1.51847, -1.51847,\n         -1.39331],\n        [-3.61956, -2.14353, -1.83888, ..., -1.49755, -1.49755,\n         -1.53033],\n        ...,\n        [-3.56643, -2.17226, -1.85591, ..., -1.52069, -1.52069,\n         -1.38938],\n        [-5.44369, -1.60174, -1.63507, ..., -1.49592, -1.49592,\n         -1.49584],\n        [-5.82887, -1.55538, -1.65036, ..., -1.58467, -1.58467,\n         -1.81844]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))observationPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='observation', length=200))Attributes: (4)created_at :2024-04-24T15:23:36.417443arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    lp               (chain, draw) float64 32kB -6.824 -4.298 ... 0.8808 -6.163\n    acceptance_rate  (chain, draw) float64 32kB 0.9944 0.7808 ... 0.9857 0.8948\n    step_size        (chain, draw) float64 32kB 0.2429 0.2429 ... 0.2209 0.2209\n    tree_depth       (chain, draw) int64 32kB 3 5 5 4 5 4 4 4 ... 4 4 5 3 4 4 4\n    n_steps          (chain, draw) int64 32kB 7 47 47 15 31 ... 31 15 15 31 15\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 11.71 22.77 15.0 ... 4.411 14.17\nAttributes:\n    created_at:                 2024-04-24T15:23:36.083541\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)lp(chain, draw)float64-6.824 -4.298 ... 0.8808 -6.163array([[-6.82444 , -4.29814 , -2.20554 , ..., -2.26909 ,  0.349555,\n        -0.503182],\n       [ 1.01036 ,  0.387115,  1.74372 , ..., -0.976692, -3.86995 ,\n        -0.648557],\n       [ 6.97037 ,  0.289142,  3.76328 , ...,  0.578271, -4.31639 ,\n        -3.73028 ],\n       [ 0.324565,  2.69654 ,  3.27323 , ...,  2.29368 ,  0.880808,\n        -6.16259 ]])acceptance_rate(chain, draw)float640.9944 0.7808 ... 0.9857 0.8948array([[0.994374, 0.780753, 0.994527, ..., 0.693056, 0.834792, 0.625842],\n       [0.927217, 0.979902, 0.975806, ..., 0.955808, 0.986919, 0.985957],\n       [0.977891, 0.829754, 0.767997, ..., 0.999606, 0.778017, 0.824205],\n       [0.961619, 0.961619, 0.965655, ..., 1.      , 0.985726, 0.894815]])step_size(chain, draw)float640.2429 0.2429 ... 0.2209 0.2209array([[0.24293 , 0.24293 , 0.24293 , ..., 0.24293 , 0.24293 , 0.24293 ],\n       [0.18297 , 0.18297 , 0.18297 , ..., 0.18297 , 0.18297 , 0.18297 ],\n       [0.192081, 0.192081, 0.192081, ..., 0.192081, 0.192081, 0.192081],\n       [0.220897, 0.220897, 0.220897, ..., 0.220897, 0.220897, 0.220897]])tree_depth(chain, draw)int643 5 5 4 5 4 4 4 ... 4 4 4 5 3 4 4 4array([[3, 5, 5, ..., 4, 4, 4],\n       [4, 4, 4, ..., 4, 4, 4],\n       [4, 4, 4, ..., 4, 4, 4],\n       [4, 4, 4, ..., 4, 4, 4]])n_steps(chain, draw)int647 47 47 15 31 31 ... 31 15 15 31 15array([[ 7, 47, 47, ..., 15, 15, 15],\n       [15, 15, 15, ..., 15, 15, 15],\n       [15, 15, 15, ..., 31, 15, 15],\n       [15, 15, 15, ..., 15, 31, 15]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float6411.71 22.77 15.0 ... 4.411 14.17array([[11.7119 , 22.7718 , 14.9989 , ...,  7.5035 , 14.19   , 13.3105 ],\n       [ 7.06501, 10.2868 , 12.36   , ..., 10.269  ,  9.39401, 10.2513 ],\n       [ 6.67939,  4.52769, 13.5245 , ...,  6.54411,  8.4759 , 13.3971 ],\n       [ 9.45526,  7.14761,  6.06554, ...,  8.34676,  4.4106 , 14.1733 ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-24T15:23:36.083541arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 26MB\nDimensions:           (chain: 4, draw: 1000, salesperson: 10, day: 5,\n                       log_lambda_dim_0: 200, observation: 200)\nCoordinates:\n  * chain             (chain) int64 32B 0 1 2 3\n  * draw              (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * salesperson       (salesperson) object 80B 'Morten Andersen' ... 'Jette T...\n  * day               (day) object 40B 'Mon' 'Tue' 'Wed' 'Thu' 'Fri'\n  * log_lambda_dim_0  (log_lambda_dim_0) int64 2kB 0 1 2 3 4 ... 196 197 198 199\n  * observation       (observation) int64 2kB 0 1 2 3 4 ... 195 196 197 198 199\nData variables:\n    log_mu            (chain, draw) float64 32kB -0.03402 1.136 ... -0.8584\n    ability           (chain, draw, salesperson) float64 320kB 0.2521 ... -0....\n    day_effect        (chain, draw, day) float64 160kB -0.5265 ... -0.6955\n    tau_ability       (chain, draw) float64 32kB 0.5254 0.4491 ... 0.1053 0.1874\n    tau_day           (chain, draw) float64 32kB 0.3982 0.6697 ... 0.674 0.4551\n    log_lambda        (chain, draw, log_lambda_dim_0) float64 6MB -0.3084 ......\n    mu                (chain, draw) float64 32kB 0.9665 3.114 ... 0.3585 0.4238\n    lambda            (chain, draw, observation) float64 6MB 0.7346 ... 0.1846\n    yrep              (chain, draw, observation) float64 6MB 1.0 0.0 ... 0.0 1.0\n    llik              (chain, draw, observation) float64 6MB -18.92 ... -4.257\nAttributes:\n    created_at:                 2024-04-24T15:23:36.409954\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000salesperson: 10day: 5log_lambda_dim_0: 200observation: 200Coordinates: (6)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])salesperson(salesperson)object'Morten Andersen' ... 'Jette Tho...array(['Morten Andersen', 'Lene Poulsen', 'Rasmus Jensen', 'Hanne Madsen',\n       'Mette Rasmussen', 'Christian Christensen', 'Helle Kristensen',\n       'Charlotte Hansen', 'Maria Petersen', 'Jette Thomsen'], dtype=object)day(day)object'Mon' 'Tue' 'Wed' 'Thu' 'Fri'array(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], dtype=object)log_lambda_dim_0(log_lambda_dim_0)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])observation(observation)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])Data variables: (10)log_mu(chain, draw)float64-0.03402 1.136 ... -1.026 -0.8584array([[-3.40241e-02,  1.13595e+00, -5.39122e-01, ..., -9.77754e-06,\n        -5.41618e-01,  1.12025e+00],\n       [-1.10820e+00,  8.47395e-01, -3.39383e-01, ..., -6.63292e-01,\n        -5.04165e-01,  1.18726e+00],\n       [-4.49574e-01,  4.86666e-01, -5.74046e-01, ...,  2.78872e+00,\n         1.76328e+00, -4.92464e-01],\n       [ 8.27251e-01, -7.73476e-01,  7.44918e-01, ..., -1.10053e+00,\n        -1.02576e+00, -8.58411e-01]])ability(chain, draw, salesperson)float640.2521 -1.141 ... 0.05478 -0.1356array([[[ 0.252114  , -1.14132   ,  0.315818  , ...,  0.448725  ,\n         -1.04776   , -1.17452   ],\n        [-0.260972  ,  0.539668  , -0.388969  , ..., -0.374155  ,\n         -0.228877  ,  0.424655  ],\n        [-0.292963  , -0.406709  , -0.203305  , ...,  0.186285  ,\n          0.173828  ,  0.417833  ],\n        ...,\n        [ 0.0724686 , -0.110269  , -0.210529  , ...,  0.281775  ,\n          0.274777  ,  0.0941643 ],\n        [-0.34881   ,  0.472278  ,  0.395984  , ..., -0.214094  ,\n         -0.903592  , -0.537416  ],\n        [ 0.2554    , -0.189946  , -0.253376  , ...,  0.041984  ,\n          1.22112   ,  0.291244  ]],\n\n       [[ 0.935444  , -0.423188  , -0.152793  , ..., -0.318619  ,\n          0.2271    ,  1.32324   ],\n        [ 0.783337  ,  0.53116   ,  0.439022  , ...,  1.12183   ,\n         -0.325948  ,  0.61191   ],\n        [-1.98739   ,  0.249339  , -0.442278  , ...,  0.692797  ,\n         -0.624501  ,  0.12784   ],\n...\n        [ 0.0441812 ,  0.0524044 ,  0.239509  , ..., -0.173234  ,\n         -0.152024  , -0.28246   ],\n        [-0.450824  , -0.0807957 , -0.233515  , ..., -0.15366   ,\n         -0.356439  , -0.360272  ],\n        [ 0.225373  ,  0.11668   ,  0.142545  , ..., -0.043435  ,\n          0.0731844 , -0.0537364 ]],\n\n       [[ 0.00970354, -0.023026  ,  0.00611605, ..., -0.866805  ,\n         -0.399008  ,  0.546029  ],\n        [-0.145619  , -0.354294  , -0.440867  , ...,  0.303382  ,\n         -0.423231  ,  0.0623959 ],\n        [ 0.040909  ,  0.506163  ,  0.438536  , ...,  0.172924  ,\n          0.808829  , -0.1091    ],\n        ...,\n        [ 0.164148  ,  0.0188097 ,  0.0449134 , ..., -0.182022  ,\n          0.0421934 , -0.0618924 ],\n        [-0.20023   , -0.0716279 ,  0.0389638 , ...,  0.165939  ,\n         -0.0442366 ,  0.102505  ],\n        [ 0.245859  ,  0.102995  , -0.0433545 , ..., -0.263572  ,\n          0.0547776 , -0.135588  ]]])day_effect(chain, draw, day)float64-0.5265 -0.4026 ... -0.255 -0.6955array([[[-5.26473e-01, -4.02648e-01, -4.73670e-01, -3.20374e-01,\n          6.44871e-01],\n        [ 4.88548e-01,  5.39676e-01,  4.97484e-01,  3.13721e-01,\n         -5.42157e-01],\n        [-5.17935e-01, -7.00982e-01, -5.70327e-01, -4.30777e-01,\n          1.04634e-01],\n        ...,\n        [-1.31590e-01, -7.45256e-01,  6.54807e-01,  2.21480e-01,\n         -3.76264e-02],\n        [-3.67335e-01, -7.11826e-01,  3.00523e-01,  4.57948e-01,\n          2.15450e-01],\n        [ 5.54835e-01,  6.75517e-01,  3.95618e-01, -2.66940e-01,\n         -3.21236e-01]],\n\n       [[ 2.08179e-01,  5.87347e-01,  3.89484e-01, -4.67304e-01,\n          6.87014e-01],\n        [-4.19887e-01, -5.50597e-01, -2.98520e-01, -1.76652e-01,\n         -3.48388e-01],\n        [-2.73157e-01,  4.38318e-01, -2.93825e-01, -1.85862e-01,\n         -1.60125e-01],\n...\n        [-5.36093e-01, -6.23134e-01,  3.90397e-01, -3.96078e-01,\n         -1.39165e+00],\n        [-4.64832e-01, -3.49328e-01,  1.00139e+00,  3.67998e-01,\n         -1.35136e+00],\n        [ 2.17470e-01, -2.27109e-01, -4.09659e-01, -2.64866e-01,\n         -2.25045e-01]],\n\n       [[ 3.87947e-01,  9.10383e-02, -1.07616e+00,  1.04234e+00,\n          8.86429e-01],\n        [-6.46909e-02, -8.12066e-02,  3.35962e-01, -2.57760e-01,\n         -1.79915e-01],\n        [-2.69486e-01,  6.34591e-02, -2.56655e-01,  7.10753e-02,\n         -1.18641e-01],\n        ...,\n        [-3.98013e-02, -1.04460e-01, -5.50450e-01, -4.29095e-01,\n         -5.84386e-01],\n        [-3.61583e-01,  1.68301e-01, -3.53333e-01, -3.26634e-01,\n         -4.94436e-01],\n        [ 1.07395e-01,  1.98965e-01,  7.77493e-02, -2.54979e-01,\n         -6.95472e-01]]])tau_ability(chain, draw)float640.5254 0.4491 ... 0.1053 0.1874array([[0.525367 , 0.44907  , 0.336569 , ..., 0.18946  , 0.57736  ,\n        0.361683 ],\n       [0.757443 , 0.745561 , 0.973322 , ..., 0.310096 , 0.34047  ,\n        0.215926 ],\n       [0.313348 , 0.241088 , 0.0699573, ..., 0.28464  , 0.389691 ,\n        0.149551 ],\n       [0.473931 , 0.226761 , 0.351934 , ..., 0.104899 , 0.105308 ,\n        0.187449 ]])tau_day(chain, draw)float640.3982 0.6697 ... 0.674 0.4551array([[0.398191 , 0.669655 , 0.464301 , ..., 0.91696  , 0.541098 ,\n        0.452506 ],\n       [0.685933 , 0.367545 , 0.259991 , ..., 0.0676436, 0.043799 ,\n        0.300327 ],\n       [0.798713 , 0.861032 , 0.585013 , ..., 0.643022 , 0.62941  ,\n        0.299095 ],\n       [0.493099 , 0.208784 , 0.206795 , ..., 0.715657 , 0.67395  ,\n        0.455114 ]])log_lambda(chain, draw, log_lambda_dim_0)float64-0.3084 -0.3084 ... -1.689 -1.689array([[[-0.308383 , -0.308383 , -0.308383 , ..., -0.563672 ,\n         -0.563672 , -0.563672 ],\n        [ 1.36353  ,  1.36353  ,  1.36353  , ...,  1.01845  ,\n          1.01845  ,  1.01845  ],\n        [-1.35002  , -1.35002  , -1.35002  , ..., -0.0166538,\n         -0.0166538, -0.0166538],\n        ...,\n        [-0.0591313, -0.0591313, -0.0591313, ...,  0.0565282,\n          0.0565282,  0.0565282],\n        [-1.25776  , -1.25776  , -1.25776  , ..., -0.863584 ,\n         -0.863584 , -0.863584 ],\n        [ 1.93048  ,  1.93048  ,  1.93048  , ...,  1.09025  ,\n          1.09025  ,  1.09025  ]],\n\n       [[ 0.0354226,  0.0354226,  0.0354226, ...,  0.902056 ,\n          0.902056 ,  0.902056 ],\n        [ 1.21085  ,  1.21085  ,  1.21085  , ...,  1.11092  ,\n          1.11092  ,  1.11092  ],\n        [-2.59993  , -2.59993  , -2.59993  , ..., -0.371669 ,\n         -0.371669 , -0.371669 ],\n...\n        [ 2.29681  ,  2.29681  ,  2.29681  , ...,  1.11461  ,\n          1.11461  ,  1.11461  ],\n        [ 0.847627 ,  0.847627 ,  0.847627 , ...,  0.0516502,\n          0.0516502,  0.0516502],\n        [-0.0496208, -0.0496208, -0.0496208, ..., -0.771246 ,\n         -0.771246 , -0.771246 ]],\n\n       [[ 1.2249   ,  1.2249   ,  1.2249   , ...,  2.25971  ,\n          2.25971  ,  2.25971  ],\n        [-0.983786 , -0.983786 , -0.983786 , ..., -0.890995 ,\n         -0.890995 , -0.890995 ],\n        [ 0.516341 ,  0.516341 ,  0.516341 , ...,  0.517177 ,\n          0.517177 ,  0.517177 ],\n        ...,\n        [-0.976179 , -0.976179 , -0.976179 , ..., -1.7468   ,\n         -1.7468   , -1.7468   ],\n        [-1.58757  , -1.58757  , -1.58757  , ..., -1.41769  ,\n         -1.41769  , -1.41769  ],\n        [-0.505157 , -0.505157 , -0.505157 , ..., -1.68947  ,\n         -1.68947  , -1.68947  ]]])mu(chain, draw)float640.9665 3.114 ... 0.3585 0.4238array([[ 0.966548,  3.11414 ,  0.58326 , ...,  0.99999 ,  0.581806,\n         3.06561 ],\n       [ 0.330153,  2.33356 ,  0.71221 , ...,  0.515153,  0.604009,\n         3.27808 ],\n       [ 0.6379  ,  1.62688 ,  0.563242, ..., 16.2602  ,  5.83155 ,\n         0.611119],\n       [ 2.28702 ,  0.461406,  2.10627 , ...,  0.332696,  0.358525,\n         0.423835]])lambda(chain, draw, observation)float640.7346 0.7346 ... 0.1846 0.1846array([[[0.734634 , 0.734634 , 0.734634 , ..., 0.569115 , 0.569115 ,\n         0.569115 ],\n        [3.90997  , 3.90997  , 3.90997  , ..., 2.7689   , 2.7689   ,\n         2.7689   ],\n        [0.259235 , 0.259235 , 0.259235 , ..., 0.983484 , 0.983484 ,\n         0.983484 ],\n        ...,\n        [0.942583 , 0.942583 , 0.942583 , ..., 1.05816  , 1.05816  ,\n         1.05816  ],\n        [0.284289 , 0.284289 , 0.284289 , ..., 0.421648 , 0.421648 ,\n         0.421648 ],\n        [6.89283  , 6.89283  , 6.89283  , ..., 2.97503  , 2.97503  ,\n         2.97503  ]],\n\n       [[1.03606  , 1.03606  , 1.03606  , ..., 2.46466  , 2.46466  ,\n         2.46466  ],\n        [3.35632  , 3.35632  , 3.35632  , ..., 3.03714  , 3.03714  ,\n         3.03714  ],\n        [0.0742788, 0.0742788, 0.0742788, ..., 0.689583 , 0.689583 ,\n         0.689583 ],\n...\n        [9.94243  , 9.94243  , 9.94243  , ..., 3.04838  , 3.04838  ,\n         3.04838  ],\n        [2.3341   , 2.3341   , 2.3341   , ..., 1.05301  , 1.05301  ,\n         1.05301  ],\n        [0.95159  , 0.95159  , 0.95159  , ..., 0.462437 , 0.462437 ,\n         0.462437 ]],\n\n       [[3.40383  , 3.40383  , 3.40383  , ..., 9.58029  , 9.58029  ,\n         9.58029  ],\n        [0.373893 , 0.373893 , 0.373893 , ..., 0.410247 , 0.410247 ,\n         0.410247 ],\n        [1.67588  , 1.67588  , 1.67588  , ..., 1.67729  , 1.67729  ,\n         1.67729  ],\n        ...,\n        [0.376748 , 0.376748 , 0.376748 , ..., 0.17433  , 0.17433  ,\n         0.17433  ],\n        [0.204422 , 0.204422 , 0.204422 , ..., 0.242274 , 0.242274 ,\n         0.242274 ],\n        [0.603411 , 0.603411 , 0.603411 , ..., 0.184617 , 0.184617 ,\n         0.184617 ]]])yrep(chain, draw, observation)float641.0 0.0 0.0 3.0 ... 1.0 0.0 0.0 1.0array([[[ 1.,  0.,  0., ...,  0.,  0.,  2.],\n        [ 5.,  7.,  4., ...,  2.,  5.,  1.],\n        [ 1.,  0.,  1., ...,  2.,  1.,  0.],\n        ...,\n        [ 0.,  3.,  2., ...,  3.,  0.,  1.],\n        [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n        [ 8.,  5.,  3., ...,  3.,  3.,  4.]],\n\n       [[ 1.,  1.,  0., ...,  1.,  4.,  5.],\n        [ 3.,  4.,  6., ...,  4.,  2.,  3.],\n        [ 0.,  0.,  1., ...,  1.,  1.,  0.],\n        ...,\n        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n        [ 1.,  0.,  4., ...,  2.,  0.,  1.],\n        [ 2.,  6.,  3., ...,  0.,  5.,  4.]],\n\n       [[ 2.,  1.,  3., ...,  1.,  0.,  0.],\n        [ 6.,  6.,  6., ...,  1.,  2.,  1.],\n        [ 3.,  2.,  3., ...,  0.,  0.,  0.],\n        ...,\n        [11.,  6., 10., ...,  6.,  3.,  4.],\n        [ 5.,  1.,  2., ...,  1.,  1.,  2.],\n        [ 0.,  1.,  0., ...,  0.,  0.,  0.]],\n\n       [[ 3.,  3.,  1., ...,  9.,  9.,  4.],\n        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n        [ 3.,  1.,  1., ...,  1.,  6.,  2.],\n        ...,\n        [ 1.,  0.,  1., ...,  0.,  1.,  0.],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n        [ 1.,  0.,  0., ...,  0.,  0.,  1.]]])llik(chain, draw, observation)float64-18.92 -3.452 ... -7.045 -4.257array([[[-18.9229 ,  -3.45154,  -5.14622, ...,  -4.05189,  -4.05189,\n          -2.38961],\n        [ -5.37909,  -1.61114,  -1.63391, ...,  -1.50531,  -1.50531,\n          -1.42515],\n        [-28.8638 ,  -6.10105,  -8.83737, ...,  -2.8252 ,  -2.8252 ,\n          -1.70994],\n        ...,\n        [-16.6383 ,  -2.91174,  -4.35716, ...,  -2.68033,  -2.68033,\n          -1.63825],\n        [-27.9663 ,  -5.84934,  -8.49339, ...,  -4.80416,  -4.80416,\n          -2.84196],\n        [ -2.69243,  -2.89314,  -2.34896, ...,  -1.49603,  -1.49603,\n          -1.48767]],\n\n       [[-15.7862 ,  -2.72155,  -4.07242, ...,  -1.55026,  -1.55026,\n          -1.3537 ],\n        [ -6.35228,  -1.51554,  -1.69099, ...,  -1.49615,  -1.49615,\n          -1.50846],\n        [-41.178  ,  -9.66583, -13.652  , ...,  -3.59635,  -3.59635,\n          -2.12607],\n...\n        [ -2.07873,  -4.84375,  -3.93324, ...,  -1.49631,  -1.49631,\n          -1.51231],\n        [ -8.96224,  -1.58298,  -2.12165, ...,  -2.68982,  -2.68982,\n          -1.64285],\n        [-16.5522 ,  -2.89221,  -4.32813, ...,  -4.56793,  -4.56793,\n          -2.69808]],\n\n       [[ -6.25923,  -1.52089,  -1.68228, ...,  -4.59293,  -4.59293,\n          -5.75402],\n        [-25.3162 ,  -5.11701,  -7.48709, ...,  -4.87499,  -4.87499,\n          -2.88538],\n        [-11.6169 ,  -1.91862,  -2.78857, ...,  -1.91751,  -1.91751,\n          -1.33608],\n        ...,\n        [-25.243  ,  -5.09704,  -7.45952, ...,  -7.2065 ,  -7.2065 ,\n          -4.36109],\n        [-31.1845 ,  -6.75889,  -9.73275, ...,  -6.28709,  -6.28709,\n          -3.7708 ],\n        [-20.7594 ,  -3.91064,  -5.80209, ...,  -7.04479,  -7.04479,\n          -4.25671]]])Indexes: (6)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))salespersonPandasIndexPandasIndex(Index(['Morten Andersen', 'Lene Poulsen', 'Rasmus Jensen', 'Hanne Madsen',\n       'Mette Rasmussen', 'Christian Christensen', 'Helle Kristensen',\n       'Charlotte Hansen', 'Maria Petersen', 'Jette Thomsen'],\n      dtype='object', name='salesperson'))dayPandasIndexPandasIndex(Index(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], dtype='object', name='day'))log_lambda_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='log_lambda_dim_0', length=200))observationPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='observation', length=200))Attributes: (4)created_at :2024-04-24T15:23:36.409954arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats_prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    lp               (chain, draw) float64 32kB -5.509 2.78 5.74 ... 14.99 13.16\n    acceptance_rate  (chain, draw) float64 32kB 0.9446 0.9892 ... 0.5428 0.2292\n    step_size        (chain, draw) float64 32kB 0.2268 0.2268 ... 0.2186 0.2186\n    tree_depth       (chain, draw) int64 32kB 4 4 4 3 4 5 4 4 ... 4 3 4 4 4 4 3\n    n_steps          (chain, draw) int64 32kB 15 15 15 7 31 ... 15 15 15 31 15\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 13.2 11.27 ... -8.691 -4.546\nAttributes:\n    created_at:                 2024-04-24T15:23:36.413130\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)lp(chain, draw)float64-5.509 2.78 5.74 ... 14.99 13.16array([[-5.50854 ,  2.78034 ,  5.73968 , ...,  8.91101 ,  1.40177 ,\n         0.101897],\n       [-6.18157 , -2.56801 , -6.73288 , ..., 14.3959  , 14.5437  ,\n        12.4763  ],\n       [ 0.969504,  6.23931 , 12.1252  , ..., -0.98214 , -0.143419,\n        11.7834  ],\n       [-5.47972 ,  5.1907  ,  8.05815 , ..., 15.2586  , 14.9928  ,\n        13.1647  ]])acceptance_rate(chain, draw)float640.9446 0.9892 ... 0.5428 0.2292array([[0.944599, 0.989201, 0.993392, ..., 0.924855, 0.656845, 0.989516],\n       [0.952239, 0.966109, 0.98479 , ..., 0.960343, 0.842611, 0.42924 ],\n       [0.56647 , 0.923299, 0.801558, ..., 0.836883, 0.974746, 0.99945 ],\n       [0.981741, 0.885162, 1.      , ..., 0.650325, 0.542819, 0.229234]])step_size(chain, draw)float640.2268 0.2268 ... 0.2186 0.2186array([[0.226821, 0.226821, 0.226821, ..., 0.226821, 0.226821, 0.226821],\n       [0.102546, 0.102546, 0.102546, ..., 0.102546, 0.102546, 0.102546],\n       [0.190488, 0.190488, 0.190488, ..., 0.190488, 0.190488, 0.190488],\n       [0.218646, 0.218646, 0.218646, ..., 0.218646, 0.218646, 0.218646]])tree_depth(chain, draw)int644 4 4 3 4 5 4 4 ... 4 4 3 4 4 4 4 3array([[4, 4, 4, ..., 3, 3, 4],\n       [6, 6, 6, ..., 5, 5, 5],\n       [4, 3, 3, ..., 5, 4, 4],\n       [5, 4, 4, ..., 4, 4, 3]])n_steps(chain, draw)int6415 15 15 7 31 31 ... 15 15 15 31 15array([[ 15,  15,  15, ...,   7,  15,  15],\n       [127,  63,  63, ...,  31,  31,  31],\n       [ 15,  15,  11, ...,  47,  15,  15],\n       [ 31,  15,  15, ...,  15,  31,  15]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float6413.2 11.27 5.336 ... -8.691 -4.546array([[13.2003 , 11.272  ,  5.33592, ...,  2.83075,  3.55562,  8.05827],\n       [21.2084 , 12.8051 , 14.2903 , ..., -1.75185, -5.78648, -4.00198],\n       [ 7.08358,  3.82209, -4.18266, ...,  7.20047, 10.6736 ,  2.04579],\n       [20.3227 , 11.2258 ,  2.25746, ..., -6.44456, -8.69135, -4.54574]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-24T15:23:36.413130arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 10kB\nDimensions:              (N_dim_0: 1, N_salesperson_dim_0: 1, N_day_dim_0: 1,\n                          salesperson_dim_0: 200, day_dim_0: 200,\n                          sales_dim_0: 200, likelihood_dim_0: 1)\nCoordinates:\n  * N_dim_0              (N_dim_0) int64 8B 0\n  * N_salesperson_dim_0  (N_salesperson_dim_0) int64 8B 0\n  * N_day_dim_0          (N_day_dim_0) int64 8B 0\n  * salesperson_dim_0    (salesperson_dim_0) int64 2kB 0 1 2 3 ... 197 198 199\n  * day_dim_0            (day_dim_0) int64 2kB 0 1 2 3 4 ... 195 196 197 198 199\n  * sales_dim_0          (sales_dim_0) int64 2kB 0 1 2 3 4 ... 196 197 198 199\n  * likelihood_dim_0     (likelihood_dim_0) int64 8B 0\nData variables:\n    N                    (N_dim_0) int64 8B 200\n    N_salesperson        (N_salesperson_dim_0) int64 8B 10\n    N_day                (N_day_dim_0) int64 8B 5\n    salesperson          (salesperson_dim_0) int64 2kB 1 1 1 1 2 ... 10 10 10 10\n    day                  (day_dim_0) int64 2kB 1 1 1 1 1 1 1 1 ... 5 5 5 5 5 5 5\n    sales                (sales_dim_0) int64 2kB 10 3 4 4 4 5 6 ... 4 1 1 3 3 2\n    likelihood           (likelihood_dim_0) int64 8B 1\nAttributes:\n    created_at:                 2024-04-24T15:23:36.415285\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:N_dim_0: 1N_salesperson_dim_0: 1N_day_dim_0: 1salesperson_dim_0: 200day_dim_0: 200sales_dim_0: 200likelihood_dim_0: 1Coordinates: (7)N_dim_0(N_dim_0)int640array([0])N_salesperson_dim_0(N_salesperson_dim_0)int640array([0])N_day_dim_0(N_day_dim_0)int640array([0])salesperson_dim_0(salesperson_dim_0)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])day_dim_0(day_dim_0)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])sales_dim_0(sales_dim_0)int640 1 2 3 4 5 ... 195 196 197 198 199array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])likelihood_dim_0(likelihood_dim_0)int640array([0])Data variables: (7)N(N_dim_0)int64200array([200])N_salesperson(N_salesperson_dim_0)int6410array([10])N_day(N_day_dim_0)int645array([5])salesperson(salesperson_dim_0)int641 1 1 1 2 2 2 ... 9 9 9 10 10 10 10array([ 1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,  4,  4,  5,\n        5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,  9,\n        9,  9, 10, 10, 10, 10,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,\n        3,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,\n        8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10,  1,  1,  1,  1,  2,\n        2,  2,  2,  3,  3,  3,  3,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,\n        6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10,\n       10,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,  4,  4,\n        5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n        9,  9,  9, 10, 10, 10, 10,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,\n        3,  3,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,\n        7,  8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10])day(day_dim_0)int641 1 1 1 1 1 1 1 ... 5 5 5 5 5 5 5 5array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5])sales(sales_dim_0)int6410 3 4 4 4 5 6 4 ... 2 4 1 1 3 3 2array([10,  3,  4,  4,  4,  5,  6,  4,  2,  4,  0,  4,  3,  3,  4,  4,  0,\n        1,  2,  1,  2,  2,  5,  6,  4,  1,  5,  3,  5,  1,  7,  0,  1,  1,\n        2,  3,  8,  6,  3,  4,  1,  3,  4,  8,  5,  8,  4,  1,  2,  0,  3,\n        1,  5,  3,  2,  3,  0,  3,  2,  0,  2,  1,  3,  2,  3,  4,  5,  3,\n        1,  1,  1,  3,  3,  1,  1,  1,  4,  4,  3,  4,  4,  3,  5,  4,  2,\n        2,  5,  4,  3,  3,  1,  2,  3,  2,  2,  2,  0,  1,  4,  1,  3,  2,\n        1,  2,  4,  4,  3,  4,  2,  1,  1,  2,  4,  1,  3,  2,  4,  3,  6,\n        4,  1,  4,  4,  0,  0,  5,  2,  3,  2,  0,  2,  1,  3,  3,  2,  3,\n        1,  1,  1,  1,  2,  2,  0,  1,  1,  3,  0,  3,  0,  1,  0,  0,  1,\n        0,  1,  1,  4,  3,  3,  1,  3,  2,  6,  2,  2,  3,  2,  3,  0,  2,\n        2,  4,  2,  2,  1,  0,  0,  2,  2,  0,  1,  2,  0,  1,  3,  3,  1,\n        0,  3,  2,  2,  0,  3,  2,  4,  1,  1,  3,  3,  2])likelihood(likelihood_dim_0)int641array([1])Indexes: (7)N_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='N_dim_0'))N_salesperson_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='N_salesperson_dim_0'))N_day_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='N_day_dim_0'))salesperson_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='salesperson_dim_0', length=200))day_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='day_dim_0', length=200))sales_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='sales_dim_0', length=200))likelihood_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='likelihood_dim_0'))Attributes: (4)created_at :2024-04-24T15:23:36.415285arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\naz.summary(idata, var_names=\"~lambda\", filter_vars=\"regex\")\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlog_mu\n0.793\n0.214\n0.389\n1.185\n0.007\n0.006\n856.0\n1075.0\n1.0\n\n\nability[Morten Andersen]\n0.440\n0.176\n0.104\n0.763\n0.005\n0.004\n1343.0\n1386.0\n1.0\n\n\nability[Lene Poulsen]\n0.376\n0.175\n0.036\n0.696\n0.005\n0.004\n1332.0\n1278.0\n1.0\n\n\nability[Rasmus Jensen]\n-0.164\n0.188\n-0.503\n0.208\n0.005\n0.004\n1568.0\n1593.0\n1.0\n\n\nability[Hanne Madsen]\n0.105\n0.183\n-0.243\n0.449\n0.005\n0.004\n1476.0\n1661.0\n1.0\n\n\nability[Mette Rasmussen]\n-0.543\n0.213\n-0.942\n-0.147\n0.005\n0.004\n1985.0\n1735.0\n1.0\n\n\nability[Christian Christensen]\n-0.122\n0.192\n-0.476\n0.253\n0.005\n0.004\n1601.0\n1829.0\n1.0\n\n\nability[Helle Kristensen]\n0.185\n0.183\n-0.153\n0.537\n0.005\n0.004\n1407.0\n1320.0\n1.0\n\n\nability[Charlotte Hansen]\n-0.278\n0.198\n-0.661\n0.091\n0.005\n0.004\n1710.0\n1663.0\n1.0\n\n\nability[Maria Petersen]\n-0.207\n0.193\n-0.582\n0.146\n0.005\n0.004\n1610.0\n1781.0\n1.0\n\n\nability[Jette Thomsen]\n0.412\n0.177\n0.103\n0.780\n0.005\n0.004\n1307.0\n1547.0\n1.0\n\n\nday_effect[Mon]\n0.326\n0.186\n-0.001\n0.662\n0.006\n0.004\n1096.0\n1317.0\n1.0\n\n\nday_effect[Tue]\n0.105\n0.188\n-0.224\n0.456\n0.006\n0.006\n1121.0\n1287.0\n1.0\n\n\nday_effect[Wed]\n0.113\n0.184\n-0.221\n0.447\n0.006\n0.005\n1122.0\n1302.0\n1.0\n\n\nday_effect[Thu]\n-0.322\n0.195\n-0.665\n0.032\n0.006\n0.006\n1226.0\n1184.0\n1.0\n\n\nday_effect[Fri]\n-0.193\n0.191\n-0.533\n0.154\n0.006\n0.006\n1218.0\n1271.0\n1.0\n\n\ntau_ability\n0.395\n0.116\n0.209\n0.617\n0.003\n0.002\n2167.0\n2615.0\n1.0\n\n\ntau_day\n0.344\n0.155\n0.123\n0.632\n0.004\n0.003\n2271.0\n1900.0\n1.0\n\n\nmu\n2.263\n0.515\n1.407\n3.142\n0.019\n0.015\n856.0\n1075.0\n1.0",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "hierarchical_models.html#the-problem-with-hierarchical-models-funnels",
    "href": "hierarchical_models.html#the-problem-with-hierarchical-models-funnels",
    "title": "Hierarchical models",
    "section": "The problem with hierarchical models: funnels",
    "text": "The problem with hierarchical models: funnels\nDid you notice that cmdstanpy printed some divergent transition warnings above? This illustrates a pervasive problem with hierarchical models: funnel-shaped marginal posterior distributions. The plot below shows the values of the parameter \\(\\tau_{day}\\) and the corresponding day effect values for Monday in the prior samples:\n\naz.plot_pair(\n    idata.prior,\n    var_names=[\"tau_day\", \"day_effect\"],\n    coords={\"day\": [\"Mon\"]},\n);\n\n/Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/.venv/lib/python3.12/site-packages/arviz/plots/pairplot.py:232: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  gridsize = int(dataset.dims[\"draw\"] ** 0.35)\n\n\n\n\n\n\n\n\n\nAs we discussed previously, funnels are hard to sample because of their inconsistent characteristic lengths. Unfortunately, they are often inevitable in hierarchical models. Do you get an idea why from the graph?\nThere are three main solutions to funnels: add more information, tune the HMC algorithm or reparameterise the model.\n\nAdd more information\nThe posterior distribution didn’t have any divergent transitions. This is probably because the extra information in the measurements made it easier to sample. Comparing the marginal distributions from above illustrates how this can happen: note that the difference in scale between the neck and the bowl of the funnel is less extreme for the posterior samples.\n\nfrom matplotlib import pyplot as plt\nf, ax = plt.subplots()\naz.plot_pair(\n    idata.prior,\n    var_names=[\"tau_day\", \"day_effect\"],\n    coords={\"day\": [\"Mon\"]},\n    ax=ax,\n    scatter_kwargs={\"label\": \"prior\"},\n);\naz.plot_pair(\n    idata.posterior,\n    var_names=[\"tau_day\", \"day_effect\"],\n    coords={\"day\": [\"Mon\"]},\n    ax=ax,\n    scatter_kwargs={\"label\": \"posterior\"},\n);\nax.legend(frameon=False);\n\n/Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/.venv/lib/python3.12/site-packages/arviz/plots/pairplot.py:232: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  gridsize = int(dataset.dims[\"draw\"] ** 0.35)\n/Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/.venv/lib/python3.12/site-packages/arviz/plots/pairplot.py:232: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  gridsize = int(dataset.dims[\"draw\"] ** 0.35)\n\n\n\n\n\n\n\n\n\nIf better measurements aren’t available, divergences can often be avoided by searching for extra information that can justify narrower priors.\n\n\nTune the algorithm\nStan allows increasing the length of the warmup phase (iter_warmup, default 2000), bringing the target acceptance probability close to 1 (adapt_delta, default 0.8) and by increasing the leapfrog integrator’s maximum tree depth (max_treedepth, default 10). All of these changes trade speed for reliability.\n\nmcmc_prior_2 = model.sample(\n    data=data_prior,\n    iter_warmup=3000,\n    adapt_delta=0.99,\n    max_treedepth=12\n)\n\n17:23:36 - cmdstanpy - INFO - CmdStan start processing\n17:23:42 - cmdstanpy - INFO - CmdStan done processing.\n17:23:42 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'plushies.stan', line 23, column 2 to column 34)\nConsider re-running with show_console=True if the above output is unclear!\n17:23:42 - cmdstanpy - WARNING - Some chains may have failed to converge.\n    Chain 4 had 1 divergent transitions (0.1%)\n    Use the \"diagnose()\" method on the CmdStanMCMC object to see further information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nUnfortunately even quite aggressive tuning doesn’t get rid of all the divergent transitions in this case.\n\n\nReparameterise\nThe idea with reparameterisation is to define auxiliary parameters which don’t have problematic relationships, then recover the problematic parameters later.\n“Non-centred” parameterisations take a distribution with the form \\(\\alpha\\sim\nD(\\mu,\\sigma)\\) and express it as follows:\n\\[\\begin{align*}\nu \\sim D(0, 1)\\\\\n\\alpha = \\mu + u * \\sigma\n\\end{align*}\\]\n\nmodel_nc = CmdStanModel(stan_file=\"../src/stan/plushies-nc.stan\")\nprint(model_nc.code())\n\ndata {\n int&lt;lower=1&gt; N;\n int&lt;lower=1&gt; N_salesperson;\n int&lt;lower=1&gt; N_day;\n array[N] int&lt;lower=1,upper=N_salesperson&gt; salesperson;\n array[N] int&lt;lower=1,upper=N_day&gt; day;\n array[N] int&lt;lower=0&gt; sales;\n int&lt;lower=0,upper=1&gt; likelihood;\n}\nparameters {\n real log_mu;\n vector[N_salesperson] ability_z;\n vector[N_day] day_effect_z;\n real&lt;lower=0&gt; tau_ability;\n real&lt;lower=0&gt; tau_day;\n}\ntransformed parameters {\n vector[N_salesperson] ability = ability_z * tau_ability;\n vector[N_day] day_effect = day_effect_z * tau_day;\n vector[N] log_lambda = log_mu + ability[salesperson] + day_effect[day]; \n}\nmodel {\n  log_mu ~ normal(0, 1);\n  ability_z ~ normal(0, 1);\n  day_effect_z ~ normal(0, 1);\n  tau_ability ~ normal(0, 1);\n  tau_day ~ normal(0, 1);\n  if (likelihood){\n    sales ~ poisson_log(log_lambda);\n  }\n}\ngenerated quantities {\n real mu = exp(log_mu);\n vector[N] lambda = exp(log_lambda);\n array[N] int yrep = poisson_rng(lambda);\n vector[N] llik; \n for (n in 1:N){\n   llik[n] = poisson_lpmf(sales[n] | lambda[n]);\n }\n}\n\n\n\n\nmcmc_prior_nc = model.sample(\n    data=data_prior,\n    iter_warmup=3000,\n    adapt_delta=0.999,\n    max_treedepth=12\n)\n\n17:23:42 - cmdstanpy - INFO - CmdStan start processing\n17:23:54 - cmdstanpy - INFO - CmdStan done processing.\n17:23:54 - cmdstanpy - WARNING - Some chains may have failed to converge.\n    Chain 2 had 2 divergent transitions (0.2%)\n    Chain 3 had 1 divergent transitions (0.1%)\n    Use the \"diagnose()\" method on the CmdStanMCMC object to see further information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nBeware of using non-centred parameterisation as a default: it isn’t guaranteed to be better.",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "hierarchical_models.html#so-how-many-plushies-do-i-need-to-sell",
    "href": "hierarchical_models.html#so-how-many-plushies-do-i-need-to-sell",
    "title": "Hierarchical models",
    "section": "So how many plushies do I need to sell?",
    "text": "So how many plushies do I need to sell?\n\nf, ax = plt.subplots()\naz.plot_forest(\n    np.exp(idata.posterior[\"log_mu\"] + idata.posterior[\"ability\"]),\n    kind=\"forestplot\",\n    combined=True,\n    ax=ax,\n    show=False,\n);\nax.scatter(\n    np.exp(np.log(BASELINE) + salespeople[\"ability\"]), \n    ax.get_yticks()[::-1], \n    color=\"red\", \n    label=\"True expected sales\",\n    zorder=2\n)\nax.scatter(\n    sales.groupby(\"salesperson\")[\"sales\"].mean().reindex(salespeople[\"salesperson\"]), \n    ax.get_yticks()[::-1], \n    color=\"black\", \n    label=\"Observed sales per day\",\n    zorder=3\n)\nax.set(title=\"\", xlabel=\"Number of plushies sold per day\")\nax.axvline(BASELINE, linestyle=\"--\", label=\"baseline\", linewidth=0.8, color=\"black\")\nax.legend(frameon=False);",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "hierarchical_models.html#takeaways",
    "href": "hierarchical_models.html#takeaways",
    "title": "Hierarchical models",
    "section": "Takeaways",
    "text": "Takeaways\n\nHierarchical models are a powerful way to capture structural information\nYou may run into problematic sampling, but you have options!\nThere is surprisingly little information in low-expected-value count data.",
    "crumbs": [
      "Course materials",
      "Hierarchical models"
    ]
  },
  {
    "objectID": "after_mcmc.html",
    "href": "after_mcmc.html",
    "title": "What to do after MCMC",
    "section": "",
    "text": "Plan for today:\nRecap from last week:\nMCMC: Monte Carlo Integration using Markov Chains\nStan: A probabilistic programming framework",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "after_mcmc.html#hatr",
    "href": "after_mcmc.html#hatr",
    "title": "What to do after MCMC",
    "section": "\\(\\hat{R}\\)",
    "text": "\\(\\hat{R}\\)\n\\(\\hat{R}\\) is a number that tells you:\n\nDo my chains agree with each other?\nAre my chains stationary?\n\n\\(\\hat{R}\\) should be close to 1. If not, you need to change something!\nFind out more: Vehtari et al. (2021)",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "after_mcmc.html#divergent-transitions",
    "href": "after_mcmc.html#divergent-transitions",
    "title": "What to do after MCMC",
    "section": "Divergent transitions",
    "text": "Divergent transitions\nThis diagnostic is specific to HMC.\nIt answers the question did the trajectory ODE solver fail?\nUsually the reason for the failure is a target distribution with very varying optimal step sizes.\nSometimes the location of the divergent transitions gives clues about the reason for the failure.\nFind out more: Betancourt (2017).",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "after_mcmc.html#retrospective-predictive-checks",
    "href": "after_mcmc.html#retrospective-predictive-checks",
    "title": "What to do after MCMC",
    "section": "Retrospective ‘predictive’ checks",
    "text": "Retrospective ‘predictive’ checks\nPredictive distribution: what a model that can replicate its measurements says about those measurements, i.e. \\(p(y^{rep})\\).\nIt’s very useful to check these things:\n\nThe prior predictive distribution should not allocate much probability mass to replicated measurements that are obviously impossible.\nIf any actual measurements lie in a region with low prior predictive probability (e.g. if measurement \\(i\\) is too low so that \\(p(y^rep_i&gt;y_i) =\n0.99\\)), that shows that the prior model is inconsistent with the measurements.\nIf there are systematic differences between the posterior predictive distribution and the observed measurements, that is a sign that the model is inconsistent with the measurements.\n\nSince predictive checking depends on pattern-matching it is often a good idea to use graphs for it.",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "after_mcmc.html#scoring-models-with-loss-functions",
    "href": "after_mcmc.html#scoring-models-with-loss-functions",
    "title": "What to do after MCMC",
    "section": "Scoring models with loss functions",
    "text": "Scoring models with loss functions\nLoss function: If the observation is \\(y\\) and the model says \\(p(y) = z\\), how bad is that?\nTo choose a model, choose a loss function, then try to minimise estimated expected loss.\n\n\n\n\n\n\nImportant\n\n\n\nWhich loss function is best depends on the problem!\n\n\nTo estimate expected loss, make some predictions.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to be useful for estimating model performance, predictions must be relevant to the evaluation context that matters.\ni.e. not from the training data, not from an already-observed sample, not from the past, etc…\n\n\nFind out more: Vehtari and Ojanen (2012)",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "after_mcmc.html#log-likelihood",
    "href": "after_mcmc.html#log-likelihood",
    "title": "What to do after MCMC",
    "section": "Log likelihood",
    "text": "Log likelihood\nA good default loss function:\n\\[\nloss(y, p(y)) = -\\ln{p(y)}\n\\]\nOut of sample log likelihood can often be approximated cheaply: see Vehtari, Gelman, and Gabry (2017).\nFind out more: (Landes and Williamson 2013, sec. 2.3)",
    "crumbs": [
      "Course materials",
      "What to do after MCMC"
    ]
  },
  {
    "objectID": "mcmc_and_stan.html",
    "href": "mcmc_and_stan.html",
    "title": "MCMC and Stan",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMCMC\nGetting started with Stan in Python\n\n\n\n\n\n\nIn A rule for evaluating the target function and maybe its gradients\nOut: A Markov Chain of numbers that you can do Monte Carlo integration with.\n\n\n\n\n\n\nOne random variable \\(X\\) with probability density function \\(density\\).\nAka a “one-dimensional parameter space”.\nEvaluating \\(density(x)\\) for a given value \\(x\\) (aka “point in parameter space”) is easy.\nCalculating the area under a region of the \\(density\\) curve (aka “the probability mass”) is expensive.\nThis is annoying, we need to know that!\n\n\n\nGenerate a series \\(x_1, ..., x_i,..., x_n\\) where every number depends on the previous number(s), i.e. a Markov chain.\nTo calculate \\(x_{i+1}\\), generate a random number and take into account \\(density(x_i)\\). 1\n1 This is the interesting and tricky bit!\nIf this works, with a long enough series of numbers we get something like this:\n\n\n\n\n\n\n\nSuccess condition for MCMC\n\n\n\nThe numbers from the Markov chain have to approximately agree with the target density function, i.e. in any region the number of dots is approximately proportional to the area under the curve.\n\n\nNow we can do Monte Carlo integration, i.e. approximate the area under a region of curve by counting the red dots in that region.\n\n\n\n\nThe first (I think?) MCMC algorithm. Original paper: Metropolis et al. (1953).2\n2 Metropolis was first author but didn’t do any work! That was Arianna Rosenbluth (programming) plus Marshall Rosenbluth & Edward Teller (maths)Generates Markov chains that provably agree with arbitrary target density functions (in the asymptotic limit).\nRoughly how it works:\n\nChoose candidate by randomly perturbing previous point \\(x_i\\)\nAccept or reject candidate randomly according to the ratio \\(\\frac{density(candidate)}{density(x_i)}\\)\n\\(x_{i+1}\\) is candidate if accept else x_i\n\nVisualisation\nDoesn’t work for more than ~10 dimensional parameter spaces.\n\n\n\nBig picture: MCMC that works for large parameter spaces.\nKey innovation: travel through parameter space quickly using gradients.\nIllustration:\n\nA better illustration\n\n\n\n\n\n\nA small but important detail:\n\n\n\nTo decide how hard to flick the ball and how precisely to calculate its trajectory for a particular case, adaptation is required, i.e. running the algorithm in warm-up mode for a bit and learning by trial and error. How best to do adaptation is an important open question.\n\n\nLimitations:\n\nNo discrete parameters\nPerforms badly when the target (log-scale) density function is wiggly.\n\n\n\nBetancourt (2018b)\nBetancourt (2018a)\nBeskos et al. (2010)\nAndrieu and Andrieu (2003)\n\n\n\n\n\nStan is:\n\nA language for specifying probability density functions as Stan programs.\nA compiler that turns Stan programs into instructions for inference engines.\nAn inference engine implementing adaptive HMC and some other algorithms.\nA library of functions for calculating the gradients of interesting probability density functions.\nSome interfaces for popular computer tools:\n\nCommand line: cmdstan\nPython:\n\ncmdstanpy\npystan\n\nR:\n\ncmdstanr\nRstan\n\n\n\n\n\nAlternatives: pymc, blackjax, Turing.jl tensorflow probability\nOverview as of 2023: Štrumbelj et al. (2023).\nWhy I like Stan:\n\nBig, active and knowledgeable community (most important reason)\nFeatureful (complex numbers, fast solvers, up-to-date diagnostics)\nFast (for CPU-bound, general purpose adaptive HMC)\n\n\n\n\n\nInstall cmdstanpy\npip install cmdstanpy\nUse cmdstanpy to install the rest of Stan\npython -m cmdstanpy.install_cmdstan --cores 2\nI like to store Stan outputs using the library arviz. It also makes nice plots.\npip install arviz\n\n\nA Stan program consists of function definitions, variable declarations and statements, organised into {...} delimited blocks, e.g.\ndata {\n  real y;  # a variable declaration\n}\nmodel {\n  y ~ normal(0, 1.4);  # a statement\n}\nThe purpose of a Stan program is to define the probability density for any combination of data and parameters.\nIt is ok for there to be no parameters:\ntransformed data {\n  real y = 2;  # this is both a statement and a declaration!\n}\nmodel {\n  y ~ normal(0, 1.4);  # the total density is N(2 | 0, 1.4) = 0.103\n}\nor no data:\nparameters {\n  real alpha;\n}\nmodel {\n  alpha ~ normal(0, 1.4);  # Stan can find the density for any alpha\n}\n\n\n\n\n\nUse standard Python tools to make a dictionary mapping data variables to inputs e.g.\nmy_stan_input = {\"y\": 2}\n(Optional) Save the input as a json file:\nimport json\nwith open(\"my_stan_input.json\", \"w\") as f:\n    json.dump(my_stan_input, f)\n\n\n\nInstantiate a CmdstanModel\nfrom cmdstanpy import CmdStanModel\nmy_model = CmdStanModel(stan_file=\"my_stan_program.stan\")\nCmdstanpy will use Stan’s compiler to create .hpp and executable files.\n\n\n\nUse the method CmdStanModel.sample to trigger adaptive HMC.\nmy_mcmc_results = my_model.sample(data=my_stan_input)\n\n\n\nUse the methods CmdStanMCMC.diagnose and CmdStanMCMC.summary for quick diagnostics.\nsummary = my_mcmc_results.summary()\ndiagnostics = my_mcmc_results.diagnose()\n\n\n\nConvert to arviz InferenceData and save\nimport arviz\nmy_idata = arviz.from_cmdstanpy(my_mcmc_results)\nmy_idata.to_json(\"my_arviz_idata.json\")\n\n\n\n\nCmdstanpy docs\nStan reference manual\nStan functions reference\nStan User’s guide\nstan-dev github organisation",
    "crumbs": [
      "Course materials",
      "MCMC and Stan"
    ]
  },
  {
    "objectID": "mcmc_and_stan.html#introduction",
    "href": "mcmc_and_stan.html#introduction",
    "title": "MCMC and Stan",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMCMC\nGetting started with Stan in Python",
    "crumbs": [
      "Course materials",
      "MCMC and Stan"
    ]
  },
  {
    "objectID": "mcmc_and_stan.html#mcmc",
    "href": "mcmc_and_stan.html#mcmc",
    "title": "MCMC and Stan",
    "section": "",
    "text": "In A rule for evaluating the target function and maybe its gradients\nOut: A Markov Chain of numbers that you can do Monte Carlo integration with.\n\n\n\n\n\n\nOne random variable \\(X\\) with probability density function \\(density\\).\nAka a “one-dimensional parameter space”.\nEvaluating \\(density(x)\\) for a given value \\(x\\) (aka “point in parameter space”) is easy.\nCalculating the area under a region of the \\(density\\) curve (aka “the probability mass”) is expensive.\nThis is annoying, we need to know that!\n\n\n\nGenerate a series \\(x_1, ..., x_i,..., x_n\\) where every number depends on the previous number(s), i.e. a Markov chain.\nTo calculate \\(x_{i+1}\\), generate a random number and take into account \\(density(x_i)\\). 1\n1 This is the interesting and tricky bit!\nIf this works, with a long enough series of numbers we get something like this:\n\n\n\n\n\n\n\nSuccess condition for MCMC\n\n\n\nThe numbers from the Markov chain have to approximately agree with the target density function, i.e. in any region the number of dots is approximately proportional to the area under the curve.\n\n\nNow we can do Monte Carlo integration, i.e. approximate the area under a region of curve by counting the red dots in that region.\n\n\n\n\nThe first (I think?) MCMC algorithm. Original paper: Metropolis et al. (1953).2\n2 Metropolis was first author but didn’t do any work! That was Arianna Rosenbluth (programming) plus Marshall Rosenbluth & Edward Teller (maths)Generates Markov chains that provably agree with arbitrary target density functions (in the asymptotic limit).\nRoughly how it works:\n\nChoose candidate by randomly perturbing previous point \\(x_i\\)\nAccept or reject candidate randomly according to the ratio \\(\\frac{density(candidate)}{density(x_i)}\\)\n\\(x_{i+1}\\) is candidate if accept else x_i\n\nVisualisation\nDoesn’t work for more than ~10 dimensional parameter spaces.\n\n\n\nBig picture: MCMC that works for large parameter spaces.\nKey innovation: travel through parameter space quickly using gradients.\nIllustration:\n\nA better illustration\n\n\n\n\n\n\nA small but important detail:\n\n\n\nTo decide how hard to flick the ball and how precisely to calculate its trajectory for a particular case, adaptation is required, i.e. running the algorithm in warm-up mode for a bit and learning by trial and error. How best to do adaptation is an important open question.\n\n\nLimitations:\n\nNo discrete parameters\nPerforms badly when the target (log-scale) density function is wiggly.\n\n\n\nBetancourt (2018b)\nBetancourt (2018a)\nBeskos et al. (2010)\nAndrieu and Andrieu (2003)",
    "crumbs": [
      "Course materials",
      "MCMC and Stan"
    ]
  },
  {
    "objectID": "mcmc_and_stan.html#stan",
    "href": "mcmc_and_stan.html#stan",
    "title": "MCMC and Stan",
    "section": "",
    "text": "Stan is:\n\nA language for specifying probability density functions as Stan programs.\nA compiler that turns Stan programs into instructions for inference engines.\nAn inference engine implementing adaptive HMC and some other algorithms.\nA library of functions for calculating the gradients of interesting probability density functions.\nSome interfaces for popular computer tools:\n\nCommand line: cmdstan\nPython:\n\ncmdstanpy\npystan\n\nR:\n\ncmdstanr\nRstan\n\n\n\n\n\nAlternatives: pymc, blackjax, Turing.jl tensorflow probability\nOverview as of 2023: Štrumbelj et al. (2023).\nWhy I like Stan:\n\nBig, active and knowledgeable community (most important reason)\nFeatureful (complex numbers, fast solvers, up-to-date diagnostics)\nFast (for CPU-bound, general purpose adaptive HMC)",
    "crumbs": [
      "Course materials",
      "MCMC and Stan"
    ]
  },
  {
    "objectID": "mcmc_and_stan.html#getting-started-with-stan-in-python",
    "href": "mcmc_and_stan.html#getting-started-with-stan-in-python",
    "title": "MCMC and Stan",
    "section": "",
    "text": "Install cmdstanpy\npip install cmdstanpy\nUse cmdstanpy to install the rest of Stan\npython -m cmdstanpy.install_cmdstan --cores 2\nI like to store Stan outputs using the library arviz. It also makes nice plots.\npip install arviz\n\n\nA Stan program consists of function definitions, variable declarations and statements, organised into {...} delimited blocks, e.g.\ndata {\n  real y;  # a variable declaration\n}\nmodel {\n  y ~ normal(0, 1.4);  # a statement\n}\nThe purpose of a Stan program is to define the probability density for any combination of data and parameters.\nIt is ok for there to be no parameters:\ntransformed data {\n  real y = 2;  # this is both a statement and a declaration!\n}\nmodel {\n  y ~ normal(0, 1.4);  # the total density is N(2 | 0, 1.4) = 0.103\n}\nor no data:\nparameters {\n  real alpha;\n}\nmodel {\n  alpha ~ normal(0, 1.4);  # Stan can find the density for any alpha\n}\n\n\n\n\n\nUse standard Python tools to make a dictionary mapping data variables to inputs e.g.\nmy_stan_input = {\"y\": 2}\n(Optional) Save the input as a json file:\nimport json\nwith open(\"my_stan_input.json\", \"w\") as f:\n    json.dump(my_stan_input, f)\n\n\n\nInstantiate a CmdstanModel\nfrom cmdstanpy import CmdStanModel\nmy_model = CmdStanModel(stan_file=\"my_stan_program.stan\")\nCmdstanpy will use Stan’s compiler to create .hpp and executable files.\n\n\n\nUse the method CmdStanModel.sample to trigger adaptive HMC.\nmy_mcmc_results = my_model.sample(data=my_stan_input)\n\n\n\nUse the methods CmdStanMCMC.diagnose and CmdStanMCMC.summary for quick diagnostics.\nsummary = my_mcmc_results.summary()\ndiagnostics = my_mcmc_results.diagnose()\n\n\n\nConvert to arviz InferenceData and save\nimport arviz\nmy_idata = arviz.from_cmdstanpy(my_mcmc_results)\nmy_idata.to_json(\"my_arviz_idata.json\")\n\n\n\n\nCmdstanpy docs\nStan reference manual\nStan functions reference\nStan User’s guide\nstan-dev github organisation",
    "crumbs": [
      "Course materials",
      "MCMC and Stan"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a course about Bayesian statistics, targeted at systems biologists.\nThere are three intended learning outcomes:\n\nUnderstand the theoretical basis for applying Bayesian data analysis to practical scientific problems\nDevelop a familiarity with implementing Bayesian data analysis using modern software tools\nGain deep understanding of both theory and practice of elements of Bayesian data analysis that are particularly relevant to computational biology, including custom hierarchical models, large analyses and statistical models with embedded ODE systems.\n\n\n\nEach week we have a one-hour seminar. The goal is to spend the time approximately as follows:\n\n25-35mins on ‘theory’, aka learning things from the book and getting more reading material\n25-35mins on practical computer work\n\n\n\n\n\n\n\n\nStatistical inference in general\nBayesian statistical inference\nThe big challenge: dimensionality\n\n\n\nSet up development environment\ngit basics\nInstall Stan and cmdstanpy\n\n\n\nJaynes (2003, Ch. 1)\nLaplace (1986)\nBox and Tiao (1992, Ch. 1.1)\n\n\n\n\n\n\nWhat is MCMC?\nHamiltonian Monte Carlo\nProbabilistic programming\n\n\n\nRun an MCMC algorithm and inspect the results\n\n\n\nBetancourt (2018)\n\n\n\n\n\n\n\n\n\nDiagnostics: convergence, divergent transitions, effective sample size\nModel evaluation as decision theory\nWhy negative log likelihood is a good default loss function\n\n\n\nDiagnose some good and bad MCMC runs\n\n\n\nVehtari et al. (2021)\nVehtari, Gelman, and Gabry (2017)\n\n\n\n\n\n\nGeneralised linear models\nPrior elicitation\nHierarchical models\n\n\n\nCompare some statistical models of a simulated biological dataset\n\n\n\nBetancourt (2024)\n\n\n\n\n\n\n\n\n\nWhat is an ODE?\nODE solvers\nODE solvers inside probabilistic programs\n\n\n\nFit a model with an ODE.\n\n\n\nTimonen et al. (2022)\n\n\n\n\n\n\nParts of a statistical anlaysis (not just inference!)\nWhy Bayesian workflow is complex: non-linearity and plurality\nWriting scalable statistical programming projects\n\n\n\nWrite a scalable statistical analysis with bibat.\n\n\n\nGelman et al. (2020)\n\n\n\n\nFormat: one hour joint feedback and help session",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#general-format",
    "href": "index.html#general-format",
    "title": "Welcome!",
    "section": "",
    "text": "Each week we have a one-hour seminar. The goal is to spend the time approximately as follows:\n\n25-35mins on ‘theory’, aka learning things from the book and getting more reading material\n25-35mins on practical computer work",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#plan",
    "href": "index.html#plan",
    "title": "Welcome!",
    "section": "",
    "text": "Statistical inference in general\nBayesian statistical inference\nThe big challenge: dimensionality\n\n\n\nSet up development environment\ngit basics\nInstall Stan and cmdstanpy\n\n\n\nJaynes (2003, Ch. 1)\nLaplace (1986)\nBox and Tiao (1992, Ch. 1.1)\n\n\n\n\n\n\nWhat is MCMC?\nHamiltonian Monte Carlo\nProbabilistic programming\n\n\n\nRun an MCMC algorithm and inspect the results\n\n\n\nBetancourt (2018)\n\n\n\n\n\n\n\n\n\nDiagnostics: convergence, divergent transitions, effective sample size\nModel evaluation as decision theory\nWhy negative log likelihood is a good default loss function\n\n\n\nDiagnose some good and bad MCMC runs\n\n\n\nVehtari et al. (2021)\nVehtari, Gelman, and Gabry (2017)\n\n\n\n\n\n\nGeneralised linear models\nPrior elicitation\nHierarchical models\n\n\n\nCompare some statistical models of a simulated biological dataset\n\n\n\nBetancourt (2024)\n\n\n\n\n\n\n\n\n\nWhat is an ODE?\nODE solvers\nODE solvers inside probabilistic programs\n\n\n\nFit a model with an ODE.\n\n\n\nTimonen et al. (2022)\n\n\n\n\n\n\nParts of a statistical anlaysis (not just inference!)\nWhy Bayesian workflow is complex: non-linearity and plurality\nWriting scalable statistical programming projects\n\n\n\nWrite a scalable statistical analysis with bibat.\n\n\n\nGelman et al. (2020)\n\n\n\n\nFormat: one hour joint feedback and help session",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html",
    "href": "introduction_to_bayesian_inference.html",
    "title": "Introduction to Bayesian inference",
    "section": "",
    "text": "What is Bayesian statistical inference?\nWhy is it useful?\nThe big challenge\n\n\n\n\nSet up git/ssh, Python, cmdstanpy and cmdstan",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#introduction",
    "href": "introduction_to_bayesian_inference.html#introduction",
    "title": "Introduction to Bayesian inference",
    "section": "",
    "text": "What is Bayesian statistical inference?\nWhy is it useful?\nThe big challenge\n\n\n\n\nSet up git/ssh, Python, cmdstanpy and cmdstan",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#probability-function",
    "href": "introduction_to_bayesian_inference.html#probability-function",
    "title": "Introduction to Bayesian inference",
    "section": "Probability function",
    "text": "Probability function\n\n\n\n\n\n\nFigure 1: A jug of water\n\n\n\nA function that can measure the water in a jug.\ni.e.\n\\(p: S \\rightarrow [0,1]\\) where\n\n\\(S\\) is an event space\nIf \\(A, B \\in S\\) are disjoint, then \\(p(A\\cup B) = p(A) + p(B)\\)",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#bayesian-epistemology",
    "href": "introduction_to_bayesian_inference.html#bayesian-epistemology",
    "title": "Introduction to Bayesian inference",
    "section": "Bayesian epistemology",
    "text": "Bayesian epistemology\nProbability functions can describe belief, e.g.\n\n\n“Definitely B”:\n\n\n“Not sure if A or B”:\n\n\n“B a bit more plausible than A”:",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#statistical-inference",
    "href": "introduction_to_bayesian_inference.html#statistical-inference",
    "title": "Introduction to Bayesian inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\n\nFigure 2: A nice soup: here is the recipe\n\n\n\nIn: facts about a spoonful sample\nOut: propositions about a soup population\ne.g.\n\nspoonful not salty \\(\\rightarrow\\) soup not salty\nno carrots in spoon \\(\\rightarrow\\) no carrots in soup",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#bayesian-statistical-inference",
    "href": "introduction_to_bayesian_inference.html#bayesian-statistical-inference",
    "title": "Introduction to Bayesian inference",
    "section": "Bayesian statistical inference",
    "text": "Bayesian statistical inference\n\n\n\n\n\n\nFigure 3: A jug of soup\n\n\n\nStatistical inference resulting in a probability.\ne.g.\n\nspoon \\(\\rightarrow\\) \\(p(\\text{soup not salty})\\) = 99.9%\nspoon \\(\\rightarrow\\) \\(p(\\text{no carrots in soup})\\) = 95.1%\n\nNon-Bayesian inferences:\n\nspoon \\(\\rightarrow\\) Best estimate of [salt] is 0.1mol/l\n\\(p_{null}(\\text{spoon})\\) = 4.9% \\(\\rightarrow\\) no carrots (p=0.049)",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#general-reasons",
    "href": "introduction_to_bayesian_inference.html#general-reasons",
    "title": "Introduction to Bayesian inference",
    "section": "General reasons",
    "text": "General reasons\n\nEasy to interpret\n\n\n\n\n\n\nFigure 4: It’s a good book!\n\n\n\nBayesian inference produces probabilities, which can be interpreted in terms of information and plausible reasoning.\ne.g. “According to the model…”\n\n“…x is highly plausible.”\n“…x is more plausible than y.”\n“…the data doesn’t contain enough information for firm conclusions about x.”\n\n\n\nOld\n\n\n\n\n\n\n(https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)\n\n\n\n\nFigure 5: Laplace, who did Bayesian inference in the 1780s\n\n\n\nBayesian inference is old!\nThis means\n\nit is well understood mathematically.\nconceptual surprises are relatively rare.\nthere are many compatible frameworks.\n\n\n\nAn easy way to represent your information\nProbabilities decompose nicely:\n\\[\np(\\theta, y) = p(\\theta)p(y\\mid\\hat{y}(\\theta))\n\\]\n\n\\(p(\\theta)\\): nice form for background information, e.g. anything non-experimental\n\\(\\hat{y}(\\theta)\\): nice form for structural information, e.g. physical laws\n\\(p(y\\mid\\hat{y}(\\theta))\\): nice form for measurement information, e.g. instrument accuracy",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#reasons-specific-to-computational-biology",
    "href": "introduction_to_bayesian_inference.html#reasons-specific-to-computational-biology",
    "title": "Introduction to Bayesian inference",
    "section": "Reasons specific to computational biology",
    "text": "Reasons specific to computational biology\n\nRegression models: good for describing measurements\nRegression: measured value noisily depends on the true value e.g. \\(y \\sim N(\\hat{y}, \\sigma)\\).\nBiology experiments often have measurement processes with awkward features. e.g.\n\nheteroskedasticity (amount of noise depends on measured value)\nconstraints (e.g. non-negativity, compositionality)\nunknown latent bias (e.g. the pump is supposed to add \\(0.05cm^3\\) per min, but does it?)\n\nBayesian inference is good at describing these.\n\n\nMulti-level models: good for describing sources of variation\n\n\n\n\n\n\nFigure 6: plot from https://github.com/teddygroves/baseball\n\n\n\nMeasurement model:\n\\(y \\sim binomial(K, logit(ability))\\)\nGpareto model:\n\\(ability \\sim GPareto(m, k, s)\\)\nNormal model:\n\\(ability \\sim N(\\mu, \\tau)\\)\n\n\nGenerative models: good for representing structural information\n\n\n\n\n\n\nFigure 7: From a Stan case study\n\n\n\nInformation about hares (\\(u\\)) and lynxes (\\(v\\)):\n\\[\\begin{align*}\n\\frac{d}{dt}u &= (\\alpha - \\beta v)u \\\\\n\\frac{d}{dt}v &= (-\\gamma + \\delta u)v\n\\end{align*}\\]\ni.e. a deterministic function turning \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), \\(u(0)\\) and \\(v(0)\\) into \\(u(t)\\) and \\(v(t)\\).",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#the-solution-mcmc",
    "href": "introduction_to_bayesian_inference.html#the-solution-mcmc",
    "title": "Introduction to Bayesian inference",
    "section": "The solution: MCMC",
    "text": "The solution: MCMC\n\n\n\n\n\n\nFigure 8: An image I found online\n\n\n\nStrategy:\n\nFind a series of numbers that\n\nquickly finds the high-probabiliy region in parameter space\nreliably matches its statistical properties\n\nDo sample-based approximate integration.\n\nIt (often) works!\nWe can tell when it doesn’t work!",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#things-to-read",
    "href": "introduction_to_bayesian_inference.html#things-to-read",
    "title": "Introduction to Bayesian inference",
    "section": "Things to read",
    "text": "Things to read\nBox and Tiao (1992, Ch. 1.1) (available from dtu findit) gives a nice explanation of statistical inference in general and why Bayes.\nHistorical interest:\n\nLaplace (1986) and Stigler (1986)\nJaynes (2003) Preface",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#things-to-set-up",
    "href": "introduction_to_bayesian_inference.html#things-to-set-up",
    "title": "Introduction to Bayesian inference",
    "section": "Things to set up",
    "text": "Things to set up\n\nPython\nFirst get a recent (ideally 3.11+) version of Python This can be very annoying so talk to me if necessary!\nNext get used to Python virtual environments.\nThe method I like is to put the virtual environment in a folder .venv inside the root of my project:\n$ python -m venv .venv --prompt=bscb\nThen to use: Tip: use an ergonomic alias to activate venvs e.g. alias va=\"source .venv/bin/activate\"\n$ source .venv/bin/activate\n# ... do work\n$ deactivate\n\n\nGit and ssh\ngit clone git@github.com:teddygroves/bayesian_statistics_for_systems_biologists.git\n\n\nCmdstanpy and cmdstan\nFirst install them:\n$ pip install cmdstanpy\n$ python -m cmdstanpy.instsall_cmdstan\nNow test if they work\nfrom cmdstanpy import CmdStanModel\nfilename = \"example_stan_program.stan\" \ncode = \"data {} parameters {real t;} model {t ~ std_normal();}\"\nwith open(filename, \"w\") as f:\n    f.write(code)\nmodel = CmdStanModel(stan_file=filename)\nmcmc = model.sample()",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#theory",
    "href": "introduction_to_bayesian_inference.html#theory",
    "title": "Introduction to Bayesian inference",
    "section": "Theory",
    "text": "Theory\nHamiltonian Monte Carlo:\n\nwhat?\nwhy?\n\nMCMC diagnostics",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "introduction_to_bayesian_inference.html#computer",
    "href": "introduction_to_bayesian_inference.html#computer",
    "title": "Introduction to Bayesian inference",
    "section": "Computer",
    "text": "Computer\nStan, cmdstanpy, arviz:\n\nformats\nworkflow\nwrite a model",
    "crumbs": [
      "Course materials",
      "Introduction to Bayesian inference"
    ]
  },
  {
    "objectID": "metropolis-hastings.html",
    "href": "metropolis-hastings.html",
    "title": "Metropolis Hastings",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMarkov Chains\nMetropolis-Hastings explained\n\nRecap from last week:\nWhat is MCMC trying to solve: MCMC",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#introduction",
    "href": "metropolis-hastings.html#introduction",
    "title": "Metropolis Hastings",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMarkov Chains\nMetropolis-Hastings explained\n\nRecap from last week:\nWhat is MCMC trying to solve: MCMC",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#mcmc",
    "href": "metropolis-hastings.html#mcmc",
    "title": "Metropolis Hastings",
    "section": "MCMC",
    "text": "MCMC",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#markov-chains",
    "href": "metropolis-hastings.html#markov-chains",
    "title": "Metropolis Hastings",
    "section": "Markov Chains",
    "text": "Markov Chains\nA Markov Chain is any process that is memoryless such that\n\\[ p(x^{i} | x^{i-1},...,x^{1}) = p(x^{i} | x^{i-1}). \\]\nFor example:\n\nThe transitions can be measured as discrete time steps with the following matrix representation\n\nimport numpy as np\nT = np.matrix([[0.1, 0.1, 0.8], [0.5, 0.1, 0.4], [0.5, 0.2, 0.3]])\nprint(T)\n\n[[0.1 0.1 0.8]\n [0.5 0.1 0.4]\n [0.5 0.2 0.3]]\n\n\nGiven an initial starting position\n\nv0 = np.matrix([0.1, 0.4, 0.5])\nprint(v0)\n\n[[0.1 0.4 0.5]]\n\n\nWe can simulate the probabilities of the next step given the transition matrix.\n\nv1 = v0*T\nprint(v1)\n\n[[0.46 0.15 0.39]]\n\n\nFollowing this again we can simulate the states after two steps\n\nv2 = v1*T\nprint(v2)\n\n[[0.316 0.139 0.545]]\n\n\nThere’s a convenient way to calculate the next step given the starting condition.\n\nprint(v0*T**2)\n\n[[0.316 0.139 0.545]]\n\n\nWhat happens if we continue doing this for a long time?\n\nprint(v0*T**100)\n\n[[0.35714286 0.14935065 0.49350649]]\n\n\nAnd how does this change when taking the next step?\n\nprint(v0*T**101)\n\n[[0.35714286 0.14935065 0.49350649]]\n\n\nThis Markov Chain has the property of being a stationary distribution. That satisfies the following\n\\[ \\pi = \\pi T. \\]\nOur objective is to estimate \\(\\pi\\), which represents the target distribution.\nThis behaviour of the Markov Chain is only satisfied if two conditions are met.\n\nThe Markov Chain is irreducible\n\nAny point in the Markov chain can be reached by any other point\n\nThe Markov Chain is aperiodic Any point can be reached by any other point in a single step",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#markov-chain-monte-carlo",
    "href": "metropolis-hastings.html#markov-chain-monte-carlo",
    "title": "Metropolis Hastings",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nIf we are able to draw samples from a Markov Chain that satisfies these properties we can generate samples from the stationary proposal distribution. After drawing samples from the sample distribution we can investigate the quantities of interest using Monte Carlo integration (read: counting samples).\nOne property that is not required for a Markov Chain but satisfies the above two properties is the detailed balance. This is not a requirement, but it’s pretty easy to define a detailed balance rather than to define general balance. This ensures that the Markov chain is reversible, in other words\n\\[ \\pi(x)*T(x'|x) = \\pi(x')*T(x|x')\\].\nIf we define a reducible process it is defined to be irreducible and aperiodic by default. It is a periodic because you can always go back, and irreducible because a region cannot be entered if there is no way of returning.\nThe process of generating a Markov Chain with these properties means that we know we are sampling from a stationary target distribution, if we have enough samples.",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#going-from-discrete-to-continuous",
    "href": "metropolis-hastings.html#going-from-discrete-to-continuous",
    "title": "Metropolis Hastings",
    "section": "Going from discrete to continuous",
    "text": "Going from discrete to continuous\nRather than the previous graph networks described before we can expand this to the continuous number line.\nNote: This isn’t always a possibility to transition between discrete and continuous number lines, it just works out for this case\nRather than sampling from \\(\\pi(x)\\), representing the discrete case, we will change the notation to \\(p(x)\\). And the transition kernel, rather than a matrix \\(T(x'|x)\\) will be represented by \\(K(x'|x).\\)\n\nMetropolis-Hastings\nMetropolis-Hastings enforces the reversibility constraint using the accept-reject function\n\\[\nA(x,x') = min(1, \\frac{p(x')g(x|x')}{p(x)g(x'|x)})\n\\]\nand often, a symmetric proposal distribution, e.g.\n\\[\ng(x'|x) = N(x, \\sigma).\n\\]\nThe resulting kernel is represented as\n\\[\nK(x'|x) = g(x'|x)*A(x,x').\n\\]\nThe accept-reject function, and the symmetric proposal distribution were chosen to satisfy the detailed balance function\n\\[\np(x)g(x'|x)A(x,x') = p(x')g(x|x')A(x,x').\n\\]\nTherefore, if we draw samples using the Metropolis-Hastings algorithm, we draw samples from the target distribution \\(p(x)\\).",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#coding-the-metropolis-hastings-in-practice",
    "href": "metropolis-hastings.html#coding-the-metropolis-hastings-in-practice",
    "title": "Metropolis Hastings",
    "section": "Coding the Metropolis Hastings in practice",
    "text": "Coding the Metropolis Hastings in practice\n\nPart 1: sampling from a normal distribution\nGiven a \\(p(x) = N(2, 0.5)\\) how would draw samples using the Metropolis-Hastings algorithm?\n\nChoose proposal value\nEvaluate probability ratio\nAccept-Reject\nincrement step\n\n\nDefine probability density function\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef prob(x):\n  return norm.pdf(x, 2, 0.5)\n\n\n\nDefine proposal distribution\n\ndef proposal(x):\n  return norm.rvs(x, 1, 1)[0]\n\n\n\nInitialise sampler\n\ncurrent = 0.0\nsamples = [current]\n\n\n\nSample from distribution\n\nfor i in range(10000):\n    prop = proposal(current)\n    accept_reject = prob(prop)/prob(current)\n    if accept_reject &gt; 1:\n        samples.append(prop)\n        current = prop\n    else:\n        cutoff = np.random.rand(1)[0]\n        if accept_reject &gt; cutoff:\n            samples.append(prop)\n            current = prop\n\n\n\nPlot distribution\n\nimport matplotlib.pyplot as plt\nplt.hist(samples)\n\n(array([   4.,   34.,  195.,  759., 1311., 1344.,  857.,  294.,   55.,\n          14.]),\n array([0.        , 0.39259336, 0.78518671, 1.17778007, 1.57037342,\n        1.96296678, 2.35556013, 2.74815349, 3.14074684, 3.5333402 ,\n        3.92593355]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nTrace plot\n\ndraw = [draw for draw, _ in enumerate(samples)]\nplt.plot(draw, samples)\n\n\n\n\n\n\n\n\n\n\n\nPart 2: determining mean and standard deviation from data\nI suggest using logs due to numerical issues. Here’s an example function which you can use to evaluate the probability of the data.\n\ndef eval_prob(data, mu, sigma):\n    return np.log(norm.pdf(data,mu,sigma)).sum()\n\nHere’s also a multivariate random number generator to generate proposals.\n\ndef proposal_multi(mu, sigma):\n    mean = [mu, sigma]\n    cov = [[0.2, 0], [0, 0.2]]  # diagonal covariance\n    return np.random.multivariate_normal(mean, cov, 1)[0]\n\nHere is how you’d call the proposal function\nprop_mu, prop_sigma = proposal_multi(current_mu, current_sigma)\nthe accept_reject probability should also be updated to account for the log-probability\naccept_reject = np.exp(prob_prop - prob_current)\nYou should sample a 95% interval including a \\(\\mu = 5\\) and a \\(\\sigma = 0.2\\). This may be difficult at first to sample and I would recommend initialising at these values.",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "metropolis-hastings.html#volume-in-hyperspace-dont-make-much-sense",
    "href": "metropolis-hastings.html#volume-in-hyperspace-dont-make-much-sense",
    "title": "Metropolis Hastings",
    "section": "Volume in hyperspace don’t make much sense",
    "text": "Volume in hyperspace don’t make much sense\nGiven an n-dimensional cube of length=2 you place spheres of diameter=1 in each of the corners and then place another sphere in the middle.\nHow does the size of the middle sphere change as dimensions increase?\n\n\nRadius of middle sphere as dimension increases",
    "crumbs": [
      "Course materials",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression models for describing measurements",
    "section": "",
    "text": "Regression is a nice way of modelling measurements.\nMaybe you have seen this regression model before:\n\\[\ny = \\alpha + X\\cdot\\beta + \\epsilon\n\\]\nComponents:\n\n\\(y\\): a measurement\n\\(\\alpha\\) and \\(\\beta\\) some unknown parameters, the same for every measurement\n\\(X\\) some real-valued features\n\\(\\epsilon\\) an unknown number quantifying the difference between the prediction and the observation, different for each measurement\n\nOften \\(\\epsilon\\) is assumed to have a normal distribution with location zero and scale parameter \\(\\sigma\\).\nAnother way of saying the same thing:\n\\[\ny \\sim N(\\alpha + X\\cdot\\beta, \\sigma)\n\\]\nI prefer this notation because it nicely separates the different components, and doesn’t hide the error parameter \\(\\sigma\\). It also makes it clear what you might change. For example \\(\\alpha + X\\cdot\\beta\\) is just one option: there are many ways in which measurements and predictors can be related. The normal distribution is also not required, in fact it is often inappropriate.\nThe key features of a regression model:\n\na predictor that is some function of the parameters and features\na probabilistic relationship between the predictor and the measurement\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the context of regression modelling, Bayesian inference lets you give free rein to your creativity when designing regressions, so you can make models that represent what you know about the measurement process.\n\n\n\n\nThis is a popular and powerful class of regression model with the following distinguishing characteristics:\n\nThe predictor is a linear function of the parameters and features, e.g.  \\(\\alpha + X\\cdot\\beta\\)\nThe probability distribution describing measurement errors is not necessarily the normal distribution, e.g. log-normal, Poisson, Dirichlet, Gamma, …\nThere is a link function that connects the linear predictor with the probability distribution, e.g. \\(\\ln(\\alpha + X\\cdot\\beta)\\).\n\nAn example GLM for positive-constrained measurements:\n\\[\ny \\sim LN(\\ln(\\alpha+X\\cdot\\beta), \\sigma)\n\\]\n\n\n\n\nStart with the simplest GLM that obeys all known data constraints.\nTry log-transforming things.\nAim to explicitly represent how you think the measurement apparatus works.\nHeavy tailed distributions are often better than the normal distribution.\nRemember that varying measurement error is an option.\n\n\n\n\n\nA practical guide to regression modelling: Gelman, Hill, and Vehtari (2020)\nThe section of the Stan user’s guide on regression models is really nice.\nModern Statistics for Modern Biology is an online (and physical) textbook with some very good material about biology-relevant regression models: Susan Holmes and Wolfgang Huber (2019).",
    "crumbs": [
      "Course materials",
      "Regression models for describing measurements"
    ]
  },
  {
    "objectID": "regression.html#what-is-regression",
    "href": "regression.html#what-is-regression",
    "title": "Regression models for describing measurements",
    "section": "",
    "text": "Regression is a nice way of modelling measurements.\nMaybe you have seen this regression model before:\n\\[\ny = \\alpha + X\\cdot\\beta + \\epsilon\n\\]\nComponents:\n\n\\(y\\): a measurement\n\\(\\alpha\\) and \\(\\beta\\) some unknown parameters, the same for every measurement\n\\(X\\) some real-valued features\n\\(\\epsilon\\) an unknown number quantifying the difference between the prediction and the observation, different for each measurement\n\nOften \\(\\epsilon\\) is assumed to have a normal distribution with location zero and scale parameter \\(\\sigma\\).\nAnother way of saying the same thing:\n\\[\ny \\sim N(\\alpha + X\\cdot\\beta, \\sigma)\n\\]\nI prefer this notation because it nicely separates the different components, and doesn’t hide the error parameter \\(\\sigma\\). It also makes it clear what you might change. For example \\(\\alpha + X\\cdot\\beta\\) is just one option: there are many ways in which measurements and predictors can be related. The normal distribution is also not required, in fact it is often inappropriate.\nThe key features of a regression model:\n\na predictor that is some function of the parameters and features\na probabilistic relationship between the predictor and the measurement\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the context of regression modelling, Bayesian inference lets you give free rein to your creativity when designing regressions, so you can make models that represent what you know about the measurement process.\n\n\n\n\nThis is a popular and powerful class of regression model with the following distinguishing characteristics:\n\nThe predictor is a linear function of the parameters and features, e.g.  \\(\\alpha + X\\cdot\\beta\\)\nThe probability distribution describing measurement errors is not necessarily the normal distribution, e.g. log-normal, Poisson, Dirichlet, Gamma, …\nThere is a link function that connects the linear predictor with the probability distribution, e.g. \\(\\ln(\\alpha + X\\cdot\\beta)\\).\n\nAn example GLM for positive-constrained measurements:\n\\[\ny \\sim LN(\\ln(\\alpha+X\\cdot\\beta), \\sigma)\n\\]\n\n\n\n\nStart with the simplest GLM that obeys all known data constraints.\nTry log-transforming things.\nAim to explicitly represent how you think the measurement apparatus works.\nHeavy tailed distributions are often better than the normal distribution.\nRemember that varying measurement error is an option.\n\n\n\n\n\nA practical guide to regression modelling: Gelman, Hill, and Vehtari (2020)\nThe section of the Stan user’s guide on regression models is really nice.\nModern Statistics for Modern Biology is an online (and physical) textbook with some very good material about biology-relevant regression models: Susan Holmes and Wolfgang Huber (2019).",
    "crumbs": [
      "Course materials",
      "Regression models for describing measurements"
    ]
  },
  {
    "objectID": "regression.html#example",
    "href": "regression.html#example",
    "title": "Regression models for describing measurements",
    "section": "Example",
    "text": "Example\nTeddy has never been in the lab and is bad at pipetting. Unfortunately, some label needed to be put in some Eppendorf tubes, and noone else was available to do it themselves or even supervise.\nEach tube had a required volume of label which Teddy tried to hit, but probably there was some inaccuracy due to bad eyesight, poor hand control or something.\nIn addition, there was also probably some consistent bias, as the pipette was consistently adding too much or too little liquid. It seemed pretty old.\nLuckily, someone was able to measure how much label ended up in 8 out of the 24 tubes with pretty good accuracy. Now we want to estimate how much label there is in the other 16 tubes, taking into account these measurements as well as what we know about the likely biased pipette and inconsistent pipetter.\nTo describe this situation we’ll first think of a regression model that describes the measurement setup, then use Python to simulate data from the model given some plausible parameter values. Next we’ll implement the model in Stan, then fit the simulated data using MCMC and then analyse the results.\n\nRegression model\nTo model the noise that Teddy introduced by being bad at pipetting and the bias introduced by the bad pipette, we need some parameters that connect the known target volumes with the unknown true volumes. Let’s call them \\(noise\\) and \\(bias\\). Since the volumes are constrained positive, a distribution that automatically excludes negative numbers is probably a good idea: the log-normal distribution is usually a good starting point. This equation describes a plausible relationship:\n\\[\nvolume \\sim LN(\\ln{(target\\cdot bias)}, noise)\n\\]\nTo model the helpful measurements, we use another log-normal distribution and assume the measuring device is unbiased and has known log-scale standard error \\(cal\\ error\\):1\n1 NB the scale parameter of a lognormal distribution represents multiplicative error\\[\nmeasurements \\sim LN(\\ln{volume}, cal\\ error)\n\\]\nTo round off the model we can think about the likely values of the unknown parameters \\(bias\\) and \\(noise\\). \\(bias\\) is likely not to be too far away from 1, otherwise someone would have probably thrown the pipette away already. A prior distribution that puts most of its mass between 0.75 and 1.25 therefore seems reasonable. Similarly, a prior for \\(noise\\) should probably not imply that Teddy’s pipetting errors regularly exceeded 30%. This consideration motivates a prior for \\(noise\\) that puts most of its mass below 0.15.\n\n\nSimulating fake data\nFirst some imports: as usual for this course we’ll be using arviz, matplotlib, cmdstanpy, pandas and numpy. stanio is a handy utility library for Stan: it is a dependency of cmdstanpy so you shouldn’t need to install it explicitly.\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport stanio\n\nfrom cmdstanpy import CmdStanModel\n\nNow some hardcoded numbers, including true values for the parameters here: \\(bias\\) is 0.88 and \\(noise\\) is 0.1. Note that \\(cal\\ error\\) is much smaller than \\(bias\\).\n\nN = 24\nN_CAL = 8\nTARGET_VOLUMES = np.array(\n    [\n      *([200] * 8),\n      *([400] * 8),\n      *([800] * 8),\n    ]\n)\nMEASUREMENT_IX = np.array([1, 4, 9, 11, 15, 19, 21, 22])\nCAL_ERROR = 0.02\nBIAS_FACTOR = 0.88\nNOISE = 0.1\nRNG_SEED = 12345\n\nSimulate the true volumes\n\nrng = np.random.default_rng(seed=RNG_SEED)\nln_mean = [\n  np.log(target * BIAS_FACTOR)\n  for target in TARGET_VOLUMES\n]\nvolumes = rng.lognormal(mean=ln_mean, sigma=NOISE)\nvolumes\n\narray([152.64294349, 199.70810807, 161.32449302, 171.49715397,\n       174.67894069, 163.43175945, 153.50063823, 187.79919405,\n       364.94147086, 289.55488638, 445.13256694, 387.79655766,\n       326.2592979 , 385.23402356, 335.94110379, 349.87019831,\n       761.78380169, 620.86368119, 745.73037525, 809.71007835,\n       803.52489045, 683.21425317, 770.52360505, 598.61585552])\n\n\nPlot the volumes and the targets.\n\nf, ax = plt.subplots()\nbins = np.logspace(np.log10(100), np.log10(1000), 30)\nax.hist(volumes, bins=bins)\nfor t in (200, 400, 800):\n    ax.axvline(t, color=\"red\")\nax.semilogx()\nax.set_xticks([200, 400, 800], [200, 400, 800]);\nax.set(\n    xlabel=\"volume ($\\\\mu$l)\",\n    ylabel=\"Frequency\",\n    title=\"How much label ended up in the tubes\"\n);\n\n\n\n\n\n\n\n\nSimulate measurements for tubes in the MEASUREMENT_IX.\n\nmeasurements = [\n  rng.lognormal(np.log(vol), CAL_ERROR)\n  for vol in volumes[MEASUREMENT_IX]\n]\npd.DataFrame({\n  \"target volume\": np.array(TARGET_VOLUMES)[MEASUREMENT_IX], \n  \"actual volume\": volumes[MEASUREMENT_IX],\n  \"measured volume\": measurements\n})\n\n\n\n\n\n\n\n\n\ntarget volume\nactual volume\nmeasured volume\n\n\n\n\n0\n200\n199.708108\n199.077273\n\n\n1\n200\n174.678941\n176.256328\n\n\n2\n400\n289.554886\n281.877576\n\n\n3\n400\n387.796558\n387.163512\n\n\n4\n400\n349.870198\n362.149468\n\n\n5\n800\n809.710078\n853.238785\n\n\n6\n800\n683.214253\n693.919342\n\n\n7\n800\n770.523605\n783.399634\n\n\n\n\n\n\n\n\n\n\nWriting the model in Stan and sampling the simulated data\nI wrote up the implied statistical model in a Stan file at src/stan/ pipette.stan. This code loads this Stan file as a CmdStanModel object, checks its formatting and prints it out.\nNote that the model internally puts the data on log scale and then standardises it: this is a bit annoying but makes it way easier to set priors and can ultimately save you a lot of trouble.\n\nmodel = CmdStanModel(stan_file=\"../src/stan/pipette.stan\")\nmodel.format(overwrite_file=True, canonicalize=True)\nprint(model.code())\n\n17:23:00 - cmdstanpy - INFO - compiling stan file /Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/src/stan/pipette.stan to exe file /Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/src/stan/pipette\n17:23:10 - cmdstanpy - INFO - compiled model executable: /Users/tedgro/repos/biosustain/bayesian_statistics_for_computational_biology/src/stan/pipette\n\n\nfunctions {\n  vector standardise(vector v, real m, real s) {\n    return (v - m) / s;\n  }\n  real standardise(real v, real m, real s) {\n    return (v - m) / s;\n  }\n  vector unstandardise(vector u, real m, real s) {\n    return m + u * s;\n  }\n  real unstandardise(real u, real m, real s) {\n    return m + u * s;\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;\n  int&lt;lower=0&gt; N_cal;\n  vector&lt;lower=0&gt;[N] target_volume;\n  vector&lt;lower=0&gt;[N_cal] y;\n  array[N_cal] int&lt;lower=1, upper=N&gt; measurement_ix;\n  real&lt;lower=0&gt; cal_error;\n  int&lt;lower=0, upper=1&gt; likelihood;\n}\ntransformed data {\n  vector[N_cal] y_ls = standardise(log(y), mean(log(y)), sd(log(y)));\n  vector[N] target_volume_ls = standardise(log(target_volume), mean(log(y)),\n                                           sd(log(y)));\n  real cal_error_s = cal_error / sd(log(y));\n}\nparameters {\n  real&lt;lower=0&gt; volume_noise_s;\n  real bias_factor_l;\n  vector[N] volume_ls;\n}\nmodel {\n  volume_noise_s ~ lognormal(log(0.1), 0.5);\n  bias_factor_l ~ normal(0, 0.15);\n  volume_ls ~ normal(target_volume_ls + bias_factor_l, volume_noise_s);\n  if (likelihood) {\n    for (i in 1 : N_cal) {\n      y_ls[i] ~ normal(volume_ls[measurement_ix[i]], cal_error_s);\n    }\n  }\n}\ngenerated quantities {\n  real bias_factor = exp(bias_factor_l);\n  real volume_noise = volume_noise_s * sd(log(y));\n  vector[N] volume = exp(unstandardise(volume_ls, mean(log(y)), sd(log(y))));\n  vector[N_cal] y_rep;\n  vector[N_cal] llik;\n  for (i in 1 : N_cal) {\n    int ms_ix = measurement_ix[i];\n    y_rep[i] = lognormal_rng(log(volume[ms_ix]), cal_error);\n    llik[i] = lognormal_lpdf(y[i] | log(volume[ms_ix]), cal_error);\n  }\n}\n\n\n\n\nThis code loads some data into a dictionary that is compatible with Stan and carries out two MCMC runs, one in prior mode and one in posterior mode.\n\nstan_input_posterior = stanio.json.process_dictionary(\n    {\n      \"N\": N,\n      \"N_cal\": N_CAL,\n      \"target_volume\": TARGET_VOLUMES,\n      \"y\": measurements,\n      \"measurement_ix\": MEASUREMENT_IX + 1,\n      \"cal_error\": CAL_ERROR,\n      \"likelihood\": 1,\n  }\n)\nstan_input_prior = stan_input_posterior | {\"likelihood\": 0}\nmcmc_prior = model.sample(\n    data=stan_input_prior,\n    adapt_delta=0.999,\n    max_treedepth=12,\n    seed=RNG_SEED,\n)\nmcmc_posterior = model.sample(data=stan_input_posterior, seed=RNG_SEED)\nmcmc_prior.diagnose()\nmcmc_posterior.diagnose()\n\n17:23:10 - cmdstanpy - INFO - CmdStan start processing\n17:23:11 - cmdstanpy - INFO - CmdStan done processing.\n17:23:11 - cmdstanpy - INFO - CmdStan start processing\n17:23:11 - cmdstanpy - INFO - CmdStan done processing.\n17:23:11 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'pipette.stan', line 38, column 2 to column 71)\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'pipette.stan', line 38, column 2 to column 71)\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'pipette.stan', line 38, column 2 to column 71)\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in 'pipette.stan', line 38, column 2 to column 71)\nConsider re-running with show_console=True if the above output is unclear!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'Processing csv files: /var/folders/ql/z_6fb5792v1_8tscf4hg5byc0000gp/T/tmpyky33fji/pipette3yjckvym/pipette-20240424172311_1.csv, /var/folders/ql/z_6fb5792v1_8tscf4hg5byc0000gp/T/tmpyky33fji/pipette3yjckvym/pipette-20240424172311_2.csv, /var/folders/ql/z_6fb5792v1_8tscf4hg5byc0000gp/T/tmpyky33fji/pipette3yjckvym/pipette-20240424172311_3.csv, /var/folders/ql/z_6fb5792v1_8tscf4hg5byc0000gp/T/tmpyky33fji/pipette3yjckvym/pipette-20240424172311_4.csv\\n\\nChecking sampler transitions treedepth.\\nTreedepth satisfactory for all transitions.\\n\\nChecking sampler transitions for divergences.\\nNo divergent transitions found.\\n\\nChecking E-BFMI - sampler transitions HMC potential energy.\\nE-BFMI satisfactory.\\n\\nEffective sample size satisfactory.\\n\\nSplit R-hat values satisfactory all parameters.\\n\\nProcessing complete, no problems detected.\\n'\n\n\nThe diagnostics seem ok, though interestingly the prior was pretty tricky to sample accurately.\n\n\nLoading the MCMC results with arviz\nThis code loads both MCMC runs into an arviz InferenceData object.\n\ncoords = {\"obs\": MEASUREMENT_IX, \"tube\": range(N)}\ndims={\n    \"y\": [\"obs\"],\n    \"y_rep\": [\"obs\"],\n    \"target_volume\": [\"tube\"],\n    \"true_volume\": [\"tube\"],\n    \"volume\": [\"tube\"],\n    \"tube\": [\"tube\"]\n}\nidata = az.from_cmdstanpy(\n    posterior=mcmc_posterior,\n    prior=mcmc_prior,\n    log_likelihood=\"llik\",\n    observed_data=stan_input_posterior | {\n        \"true_volume\": volumes, \"tube\": range(N)\n    },\n    posterior_predictive={\"y\": \"y_rep\"},\n    coords=coords,\n    dims=dims\n)\nidata\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:          (chain: 4, draw: 1000, volume_ls_dim_0: 24, tube: 24)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * volume_ls_dim_0  (volume_ls_dim_0) int64 192B 0 1 2 3 4 5 ... 19 20 21 22 23\n  * tube             (tube) int64 192B 0 1 2 3 4 5 6 7 ... 17 18 19 20 21 22 23\nData variables:\n    volume_noise_s   (chain, draw) float64 32kB 0.149 0.2378 ... 0.1396 0.1959\n    bias_factor_l    (chain, draw) float64 32kB -0.2014 -0.07906 ... -0.07839\n    volume_ls        (chain, draw, volume_ls_dim_0) float64 768kB -1.275 ... ...\n    bias_factor      (chain, draw) float64 32kB 0.8176 0.924 ... 0.9842 0.9246\n    volume_noise     (chain, draw) float64 32kB 0.09116 0.1455 ... 0.1199\n    volume           (chain, draw, tube) float64 768kB 182.9 192.1 ... 855.2\nAttributes:\n    created_at:                 2024-04-24T15:23:11.847386\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000volume_ls_dim_0: 24tube: 24Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])volume_ls_dim_0(volume_ls_dim_0)int640 1 2 3 4 5 6 ... 18 19 20 21 22 23array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23])tube(tube)int640 1 2 3 4 5 6 ... 18 19 20 21 22 23array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23])Data variables: (6)volume_noise_s(chain, draw)float640.149 0.2378 ... 0.1396 0.1959array([[0.148979, 0.237794, 0.122436, ..., 0.140801, 0.178431, 0.198993],\n       [0.158124, 0.141742, 0.157009, ..., 0.167521, 0.137968, 0.115493],\n       [0.168817, 0.188685, 0.138985, ..., 0.138549, 0.116906, 0.133163],\n       [0.151583, 0.148293, 0.18479 , ..., 0.158791, 0.139599, 0.195866]])bias_factor_l(chain, draw)float64-0.2014 -0.07906 ... -0.07839array([[-0.201351 , -0.0790569, -0.105641 , ..., -0.123153 , -0.0710773,\n        -0.0700885],\n       [-0.0982885, -0.0746649, -0.0754556, ..., -0.128265 , -0.163382 ,\n        -0.123952 ],\n       [ 0.0213591, -0.182919 , -0.17903  , ..., -0.134593 , -0.132122 ,\n        -0.131316 ],\n       [-0.0805919, -0.0926586, -0.117743 , ..., -0.190522 , -0.0158839,\n        -0.0783902]])volume_ls(chain, draw, volume_ls_dim_0)float64-1.275 -1.194 ... 1.099 1.246array([[[-1.2747  , -1.19448 , -1.22282 , ...,  0.879734,  1.03417 ,\n          0.762953],\n        [-1.40687 , -1.09754 , -1.14365 , ...,  0.936603,  1.15697 ,\n          0.907036],\n        [-0.935161, -1.16238 , -1.13239 , ...,  0.901988,  1.09336 ,\n          1.1396  ],\n        ...,\n        [-1.38442 , -1.08361 , -1.21375 , ...,  0.945476,  1.12959 ,\n          1.01907 ],\n        [-1.03524 , -1.11487 , -0.966034, ...,  0.964533,  1.11521 ,\n          1.0335  ],\n        [-1.29565 , -1.14228 , -1.38558 , ...,  0.843054,  1.08359 ,\n          1.15616 ]],\n\n       [[-1.28813 , -1.15007 , -1.30488 , ...,  0.863103,  1.06637 ,\n          1.34264 ],\n        [-1.17917 , -1.1655  , -1.16521 , ...,  0.874006,  1.07137 ,\n          1.25143 ],\n        [-1.20124 , -1.14406 , -1.0035  , ...,  0.908254,  1.06948 ,\n          1.04256 ],\n...\n        [-1.28454 , -1.16143 , -1.07814 , ...,  0.871924,  1.07582 ,\n          1.04087 ],\n        [-1.35706 , -1.15124 , -1.11947 , ...,  0.87177 ,  1.04045 ,\n          0.944458],\n        [-1.23871 , -1.14033 , -1.48308 , ...,  0.895505,  1.1146  ,\n          0.996843]],\n\n       [[-1.21437 , -1.11281 , -1.31588 , ...,  0.869607,  1.11421 ,\n          0.911445],\n        [-1.23622 , -1.12363 , -1.30974 , ...,  0.940612,  1.11077 ,\n          1.07039 ],\n        [-1.15205 , -1.14246 , -1.21216 , ...,  0.914147,  1.09383 ,\n          1.10271 ],\n        ...,\n        [-1.28744 , -1.10417 , -1.22327 , ...,  0.90872 ,  1.1949  ,\n          1.00884 ],\n        [-1.19644 , -1.16323 , -1.25646 , ...,  0.913467,  1.0723  ,\n          1.06841 ],\n        [-0.985007, -1.08883 , -1.03812 , ...,  0.941881,  1.09928 ,\n          1.24558 ]]])bias_factor(chain, draw)float640.8176 0.924 ... 0.9842 0.9246array([[0.817625, 0.923987, 0.899748, ..., 0.884128, 0.93139 , 0.932311],\n       [0.906387, 0.928054, 0.927321, ..., 0.87962 , 0.849267, 0.883422],\n       [1.02159 , 0.832835, 0.836081, ..., 0.874071, 0.876234, 0.876941],\n       [0.92257 , 0.911505, 0.888924, ..., 0.826528, 0.984242, 0.924604]])volume_noise(chain, draw)float640.09116 0.1455 ... 0.08542 0.1199array([[0.0911635, 0.145511 , 0.0749212, ..., 0.0861592, 0.109185 ,\n        0.121768 ],\n       [0.0967594, 0.0867349, 0.0960768, ..., 0.102509 , 0.0844255,\n        0.0706728],\n       [0.103303 , 0.11546  , 0.0850475, ..., 0.0847808, 0.0715373,\n        0.0814854],\n       [0.092757 , 0.0907435, 0.113077 , ..., 0.0971674, 0.0854233,\n        0.119854 ]])volume(chain, draw, tube)float64182.9 192.1 188.8 ... 782.0 855.2array([[[182.932, 192.136, 188.833, ..., 683.658, 751.418, 636.508],\n        [168.719, 203.878, 198.206, ..., 707.868, 810.057, 695.176],\n        [225.177, 195.948, 199.577, ..., 693.032, 779.132, 801.494],\n        ...,\n        [171.054, 205.624, 189.884, ..., 711.722, 796.6  , 744.505],\n        [211.801, 201.728, 220.963, ..., 720.07 , 789.621, 751.109],\n        [180.602, 198.373, 170.932, ..., 668.484, 774.49 , 809.654]],\n\n       [[181.435, 197.428, 179.585, ..., 676.736, 766.367, 907.524],\n        [193.944, 195.574, 195.609, ..., 681.266, 768.718, 858.258],\n        [191.343, 198.157, 215.955, ..., 695.694, 767.829, 755.286],\n        ...,\n        [179.235, 204.075, 162.611, ..., 709.551, 781.422, 723.426],\n        [189.975, 192.176, 176.443, ..., 714.348, 760.103, 754.809],\n        [186.767, 198.987, 204.561, ..., 705.813, 787.317, 706.511]],\n\n       [[198.651, 208.316, 215.697, ..., 682.152, 770.541, 821.472],\n        [175.866, 190.675, 167.71 , ..., 713.088, 788.866, 644.97 ],\n        [185.411, 195.48 , 169.569, ..., 690.813, 788.233, 665.731],\n        ...,\n        [181.834, 196.062, 206.313, ..., 680.399, 770.813, 754.506],\n        [173.941, 197.288, 201.161, ..., 680.335, 754.311, 711.279],\n        [187.005, 198.61 , 161.032, ..., 690.288, 789.322, 734.448]],\n\n       [[189.812, 201.983, 178.38 , ..., 679.435, 789.135, 697.054],\n        [187.291, 200.65 , 179.052, ..., 709.606, 787.476, 768.257],\n        [197.19 , 198.351, 190.069, ..., 698.207, 779.357, 783.604],\n        ...,\n        [181.511, 203.053, 188.781, ..., 695.893, 829.077, 739.861],\n        [191.905, 195.846, 184.986, ..., 697.917, 769.157, 767.329],\n        [218.412, 204.968, 211.428, ..., 710.158, 781.96 , 855.193]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))volume_ls_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23],\n      dtype='int64', name='volume_ls_dim_0'))tubePandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23],\n      dtype='int64', name='tube'))Attributes: (4)created_at :2024-04-24T15:23:11.847386arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 264kB\nDimensions:  (chain: 4, draw: 1000, obs: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * obs      (obs) int64 64B 1 4 9 11 15 19 21 22\nData variables:\n    y_rep    (chain, draw, obs) float64 256kB 197.1 177.7 290.6 ... 693.9 779.7\nAttributes:\n    created_at:                 2024-04-24T15:23:11.853320\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000obs: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int641 4 9 11 15 19 21 22array([ 1,  4,  9, 11, 15, 19, 21, 22])Data variables: (1)y_rep(chain, draw, obs)float64197.1 177.7 290.6 ... 693.9 779.7array([[[197.116, 177.676, 290.624, ..., 817.525, 684.834, 768.599],\n        [207.123, 178.881, 276.029, ..., 837.783, 707.797, 799.808],\n        [193.412, 176.359, 312.075, ..., 779.207, 670.578, 748.349],\n        ...,\n        [205.396, 181.619, 282.37 , ..., 823.026, 693.464, 776.76 ],\n        [196.261, 179.431, 277.182, ..., 842.408, 693.157, 799.594],\n        [194.616, 187.719, 282.712, ..., 827.156, 681.762, 760.655]],\n\n       [[194.801, 174.147, 275.591, ..., 837.393, 662.098, 793.396],\n        [196.473, 175.265, 281.09 , ..., 849.397, 685.798, 799.947],\n        [188.093, 169.629, 275.093, ..., 851.105, 699.691, 758.693],\n        ...,\n        [199.97 , 181.119, 276.119, ..., 885.826, 721.481, 780.279],\n        [190.317, 177.701, 271.189, ..., 838.828, 746.589, 728.727],\n        [204.84 , 172.027, 285.739, ..., 834.172, 716.608, 793.402]],\n\n       [[208.713, 177.249, 294.043, ..., 808.862, 703.655, 795.465],\n        [195.615, 175.848, 270.427, ..., 868.339, 708.692, 818.016],\n        [196.822, 173.831, 290.472, ..., 826.401, 681.787, 797.722],\n        ...,\n        [193.524, 174.954, 287.664, ..., 903.736, 662.875, 770.548],\n        [198.162, 176.923, 293.263, ..., 855.101, 686.138, 768.147],\n        [195.309, 174.313, 283.913, ..., 858.254, 697.01 , 809.608]],\n\n       [[206.681, 178.897, 283.43 , ..., 877.47 , 671.592, 773.122],\n        [204.176, 182.882, 273.709, ..., 802.932, 712.307, 766.771],\n        [198.835, 175.814, 295.483, ..., 807.124, 708.253, 755.885],\n        ...,\n        [199.387, 166.869, 282.976, ..., 836.791, 687.756, 831.191],\n        [189.255, 178.416, 288.503, ..., 861.03 , 712.108, 784.301],\n        [207.587, 184.839, 288.502, ..., 839.977, 693.941, 779.685]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))obsPandasIndexPandasIndex(Index([1, 4, 9, 11, 15, 19, 21, 22], dtype='int64', name='obs'))Attributes: (4)created_at :2024-04-24T15:23:11.853320arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 264kB\nDimensions:     (chain: 4, draw: 1000, llik_dim_0: 8)\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * llik_dim_0  (llik_dim_0) int64 64B 0 1 2 3 4 5 6 7\nData variables:\n    llik        (chain, draw, llik_dim_0) float64 256kB -3.875 -2.193 ... -3.675\nAttributes:\n    created_at:                 2024-04-24T15:23:11.894718\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000llik_dim_0: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])llik_dim_0(llik_dim_0)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])Data variables: (1)llik(chain, draw, llik_dim_0)float64-3.875 -2.193 ... -4.218 -3.675array([[[-3.87496, -2.19298, -5.05717, ..., -4.10576, -3.82668,\n         -5.84213],\n        [-3.0104 , -2.18047, -2.95307, ..., -3.76203, -4.04436,\n         -5.07012],\n        [-2.61437, -2.19405, -4.63408, ..., -3.87112, -3.55132,\n         -3.70786],\n        ...,\n        [-3.60909, -2.48505, -2.6706 , ..., -3.91375, -4.35139,\n         -4.01956],\n        [-2.51933, -2.83159, -2.7908 , ..., -3.79013, -5.25984,\n         -3.74878],\n        [-2.31632, -4.73471, -3.46818, ..., -5.85992, -5.29236,\n         -3.8341 ]],\n\n       [[-2.38707, -2.3283 , -2.64878, ..., -3.83624, -4.33519,\n         -4.27453],\n        [-2.69467, -2.22771, -2.68636, ..., -3.96266, -3.97258,\n         -4.11793],\n        [-2.32746, -2.27031, -4.41154, ..., -4.53482, -3.55743,\n         -4.17434],\n...\n        [-2.59181, -2.25385, -2.67099, ..., -4.91015, -4.03322,\n         -3.99848],\n        [-2.40251, -2.43096, -4.67866, ..., -4.25938, -4.03786,\n         -5.46025],\n        [-2.30753, -3.00209, -2.83217, ..., -3.75844, -3.58368,\n         -3.74147]],\n\n       [[-2.56303, -2.25859, -3.31605, ..., -3.90128, -4.10547,\n         -3.73708],\n        [-2.37797, -2.74359, -4.55847, ..., -4.38751, -4.17394,\n         -3.70423],\n        [-2.31732, -2.66832, -4.37736, ..., -3.80292, -3.59671,\n         -3.70401],\n        ...,\n        [-2.78946, -2.22266, -2.67603, ..., -3.75926, -3.55935,\n         -7.68485],\n        [-2.63537, -2.2518 , -3.67463, ..., -4.03551, -3.59052,\n         -4.09134],\n        [-3.36342, -2.82215, -3.077  , ..., -3.84831, -4.21812,\n         -3.67479]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))llik_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='llik_dim_0'))Attributes: (4)created_at :2024-04-24T15:23:11.894718arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    lp               (chain, draw) float64 32kB 21.01 18.71 ... 24.94 22.19\n    acceptance_rate  (chain, draw) float64 32kB 0.9988 0.7904 ... 0.8739 0.9229\n    step_size        (chain, draw) float64 32kB 0.3503 0.3503 ... 0.4368 0.4368\n    tree_depth       (chain, draw) int64 32kB 3 4 3 3 3 3 3 3 ... 3 3 3 3 3 3 4\n    n_steps          (chain, draw) int64 32kB 7 31 7 15 7 7 7 ... 7 7 7 7 7 7 15\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB -6.192 -8.972 ... -15.39 -11.6\nAttributes:\n    created_at:                 2024-04-24T15:23:11.851799\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)lp(chain, draw)float6421.01 18.71 28.67 ... 24.94 22.19array([[21.009 , 18.71  , 28.6744, ..., 24.9497, 21.3791, 18.4294],\n       [27.8086, 32.2878, 26.6081, ..., 26.1025, 28.0347, 28.8471],\n       [24.4864, 25.8458, 31.4533, ..., 31.4475, 30.7953, 30.7299],\n       [30.3395, 21.0069, 20.8335, ..., 24.7186, 24.9444, 22.194 ]])acceptance_rate(chain, draw)float640.9988 0.7904 1.0 ... 0.8739 0.9229array([[0.998827, 0.790351, 1.      , ..., 0.935358, 0.971577, 0.847244],\n       [0.979632, 0.984735, 0.751846, ..., 1.      , 0.901231, 0.83027 ],\n       [0.64411 , 0.901806, 0.983725, ..., 0.809004, 0.692338, 0.828195],\n       [0.931977, 0.618653, 0.940008, ..., 0.973274, 0.873885, 0.922928]])step_size(chain, draw)float640.3503 0.3503 ... 0.4368 0.4368array([[0.350264, 0.350264, 0.350264, ..., 0.350264, 0.350264, 0.350264],\n       [0.343679, 0.343679, 0.343679, ..., 0.343679, 0.343679, 0.343679],\n       [0.326612, 0.326612, 0.326612, ..., 0.326612, 0.326612, 0.326612],\n       [0.43677 , 0.43677 , 0.43677 , ..., 0.43677 , 0.43677 , 0.43677 ]])tree_depth(chain, draw)int643 4 3 3 3 3 3 3 ... 3 3 3 3 3 3 3 4array([[3, 4, 3, ..., 4, 4, 4],\n       [3, 3, 4, ..., 4, 4, 3],\n       [4, 4, 4, ..., 3, 4, 3],\n       [3, 3, 3, ..., 3, 3, 4]])n_steps(chain, draw)int647 31 7 15 7 7 7 ... 7 7 7 7 7 7 15array([[ 7, 31,  7, ..., 15, 15, 15],\n       [ 7,  7, 15, ..., 15, 31,  7],\n       [15, 15, 15, ...,  7, 15, 15],\n       [ 7,  7,  7, ...,  7,  7, 15]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64-6.192 -8.972 ... -15.39 -11.6array([[ -6.19192,  -8.97176, -14.139  , ..., -11.2611 , -12.0468 ,\n         -1.39412],\n       [-13.6371 , -23.0408 , -15.4574 , ...,  -7.31748, -10.7674 ,\n        -13.1799 ],\n       [-14.2463 , -12.2545 , -20.9744 , ..., -15.1772 , -12.6979 ,\n        -18.2411 ],\n       [-17.2553 , -11.9235 ,  -9.46781, ..., -16.5739 , -15.3934 ,\n        -11.6042 ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-24T15:23:11.851799arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:          (chain: 4, draw: 1000, volume_ls_dim_0: 24, tube: 24,\n                      obs: 8, llik_dim_0: 8)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * volume_ls_dim_0  (volume_ls_dim_0) int64 192B 0 1 2 3 4 5 ... 19 20 21 22 23\n  * tube             (tube) int64 192B 0 1 2 3 4 5 6 7 ... 17 18 19 20 21 22 23\n  * obs              (obs) int64 64B 1 4 9 11 15 19 21 22\n  * llik_dim_0       (llik_dim_0) int64 64B 0 1 2 3 4 5 6 7\nData variables:\n    volume_noise_s   (chain, draw) float64 32kB 0.02974 0.02728 ... 0.2746\n    bias_factor_l    (chain, draw) float64 32kB -0.1025 -0.07485 ... -0.1921\n    volume_ls        (chain, draw, volume_ls_dim_0) float64 768kB -1.18 ... 0...\n    bias_factor      (chain, draw) float64 32kB 0.9026 0.9279 ... 0.9219 0.8252\n    volume_noise     (chain, draw) float64 32kB 0.0182 0.01669 ... 0.1236 0.168\n    volume           (chain, draw, tube) float64 768kB 193.8 185.0 ... 693.1\n    y_rep            (chain, draw, obs) float64 256kB 184.8 189.1 ... 1.002e+03\n    llik             (chain, draw, llik_dim_0) float64 256kB -8.981 ... -87.32\nAttributes:\n    created_at:                 2024-04-24T15:23:11.889104\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000volume_ls_dim_0: 24tube: 24obs: 8llik_dim_0: 8Coordinates: (6)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])volume_ls_dim_0(volume_ls_dim_0)int640 1 2 3 4 5 6 ... 18 19 20 21 22 23array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23])tube(tube)int640 1 2 3 4 5 6 ... 18 19 20 21 22 23array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23])obs(obs)int641 4 9 11 15 19 21 22array([ 1,  4,  9, 11, 15, 19, 21, 22])llik_dim_0(llik_dim_0)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])Data variables: (8)volume_noise_s(chain, draw)float640.02974 0.02728 ... 0.2021 0.2746array([[0.0297445, 0.0272758, 0.0265095, ..., 0.178736 , 0.192878 ,\n        0.143125 ],\n       [0.0217743, 0.0275153, 0.0318496, ..., 0.187067 , 0.213967 ,\n        0.2151   ],\n       [0.14644  , 0.161344 , 0.154124 , ..., 0.222906 , 0.210747 ,\n        0.183849 ],\n       [0.196802 , 0.224444 , 0.332829 , ..., 0.104148 , 0.202053 ,\n        0.274571 ]])bias_factor_l(chain, draw)float64-0.1025 -0.07485 ... -0.1921array([[-0.102507 , -0.074851 , -0.071183 , ..., -0.0135833, -0.072937 ,\n        -0.0888987],\n       [-0.131403 , -0.137815 , -0.131853 , ..., -0.0773899,  0.0578722,\n        -0.0226735],\n       [ 0.134711 ,  0.0641626, -0.0368078, ..., -0.277545 , -0.22355  ,\n        -0.29311  ],\n       [-0.0320055, -0.127452 ,  0.0724585, ..., -0.0552708, -0.0813462,\n        -0.192088 ]])volume_ls(chain, draw, volume_ls_dim_0)float64-1.18 -1.256 ... 1.525 0.9022array([[[-1.18014 , -1.25596 , -1.21539 , ...,  1.06275 ,  1.04836 ,\n          1.03579 ],\n        [-1.24896 , -1.18338 , -1.21905 , ...,  1.03778 ,  1.05845 ,\n          1.05922 ],\n        [-1.18062 , -1.20861 , -1.20187 , ...,  1.06469 ,  1.05634 ,\n          1.01356 ],\n        ...,\n        [-1.06469 , -1.19225 , -1.39826 , ...,  0.878588,  1.39824 ,\n          1.34754 ],\n        [-1.43398 , -1.40853 , -1.21943 , ...,  1.50548 ,  0.928725,\n          0.986833],\n        [-1.19609 , -1.28651 , -1.26602 , ...,  0.972567,  1.07923 ,\n          1.10071 ]],\n\n       [[-1.24692 , -1.24356 , -1.21965 , ...,  1.00025 ,  0.983762,\n          1.02885 ],\n        [-1.28328 , -1.29047 , -1.25435 , ...,  1.07908 ,  0.979001,\n          0.99518 ],\n        [-1.24983 , -1.2417  , -1.28149 , ...,  0.913942,  1.01969 ,\n          1.0017  ],\n...\n        [-1.28413 , -1.1961  , -1.73514 , ...,  1.07113 ,  1.08167 ,\n          0.796746],\n        [-1.53629 , -1.42865 , -1.07195 , ...,  0.73767 ,  0.848461,\n          1.2325  ],\n        [-1.33974 , -1.18125 , -1.43554 , ...,  0.932523,  0.996232,\n          0.864258]],\n\n       [[-1.04915 , -0.949361, -0.929134, ...,  1.15517 ,  0.924563,\n          0.388474],\n        [-1.26699 , -1.06466 , -1.12933 , ...,  0.873924,  0.755863,\n          1.21534 ],\n        [-1.1652  , -1.53275 , -1.46456 , ...,  1.52952 ,  1.5088  ,\n          1.27139 ],\n        ...,\n        [-1.14013 , -1.28078 , -1.14028 , ...,  1.01727 ,  1.10426 ,\n          0.960508],\n        [-1.4791  , -1.09886 , -1.39305 , ...,  0.918405,  1.08038 ,\n          1.22328 ],\n        [-1.2026  , -1.02384 , -1.02024 , ...,  1.01766 ,  1.52503 ,\n          0.902215]]])bias_factor(chain, draw)float640.9026 0.9279 ... 0.9219 0.8252array([[0.902572, 0.927882, 0.931291, ..., 0.986509, 0.929659, 0.914938],\n       [0.876865, 0.87126 , 0.87647 , ..., 0.925529, 1.05958 , 0.977582],\n       [1.14421 , 1.06627 , 0.963861, ..., 0.757641, 0.799675, 0.74594 ],\n       [0.968501, 0.880336, 1.07515 , ..., 0.946229, 0.921874, 0.825234]])volume_noise(chain, draw)float640.0182 0.01669 ... 0.1236 0.168array([[0.0182013, 0.0166906, 0.0162217, ..., 0.109372 , 0.118026 ,\n        0.0875808],\n       [0.0133241, 0.0168372, 0.0194894, ..., 0.11447  , 0.130931 ,\n        0.131624 ],\n       [0.0896095, 0.0987295, 0.0943116, ..., 0.136401 , 0.12896  ,\n        0.112501 ],\n       [0.120427 , 0.137342 , 0.203665 , ..., 0.0637301, 0.12364  ,\n        0.168016 ]])volume(chain, draw, tube)float64193.8 185.0 ... 1.015e+03 693.1array([[[ 193.83 ,  185.043,  189.693, ...,  764.673,  757.97 ,\n          752.164],\n        [ 185.837,  193.446,  189.27 , ...,  753.078,  762.666,\n          763.023],\n        [ 193.772,  190.482,  191.27 , ...,  765.581,  761.682,\n          742.002],\n        ...,\n        [ 208.018,  192.399,  169.611, ...,  683.179,  938.933,\n          910.251],\n        [ 165.943,  168.549,  189.225, ..., 1002.61 ,  704.464,\n          729.963],\n        [ 191.947,  181.615,  183.906, ...,  723.619,  772.422,\n          782.645]],\n\n       [[ 186.069,  186.452,  189.2  , ...,  735.982,  728.593,\n          748.973],\n        [ 181.974,  181.175,  185.225, ...,  772.355,  726.473,\n          733.701],\n        [ 185.738,  186.664,  182.174, ...,  698.12 ,  744.787,\n          736.633],\n...\n        [ 181.879,  191.946,  138.015, ...,  768.607,  773.58 ,\n          649.808],\n        [ 155.873,  166.485,  207.096, ...,  626.737,  670.7  ,\n          848.373],\n        [ 175.795,  193.698,  165.786, ...,  706.103,  734.174,\n          677.215]],\n\n       [[ 210.005,  223.229,  226.009, ...,  809.167,  702.672,\n          506.157],\n        [ 183.797,  208.022,  199.951, ...,  681.232,  633.753,\n          839.513],\n        [ 195.61 ,  156.211,  162.867, ..., 1017.47 , 1004.65 ,\n          868.806],\n        ...,\n        [ 198.634,  182.253,  198.615, ...,  743.688,  784.348,\n          718.299],\n        [ 161.425,  203.714,  170.153, ...,  700.029,  772.966,\n          843.605],\n        [ 191.183,  213.283,  213.753, ...,  743.862, 1014.68 ,\n          693.128]]])y_rep(chain, draw, obs)float64184.8 189.1 ... 741.0 1.002e+03array([[[ 184.813,  189.054,  386.057, ...,  759.48 ,  754.107,\n          752.463],\n        [ 188.057,  194.853,  377.611, ...,  747.653,  741.944,\n          791.905],\n        [ 191.471,  191.019,  380.344, ...,  762.074,  750.234,\n          764.223],\n        ...,\n        [ 188.237,  217.168,  422.702, ...,  804.179,  687.31 ,\n          936.802],\n        [ 167.074,  207.593,  425.475, ...,  681.052, 1021.48 ,\n          705.951],\n        [ 177.321,  174.68 ,  365.093, ...,  756.83 ,  745.255,\n          747.625]],\n\n       [[ 194.119,  186.317,  365.097, ...,  722.354,  724.962,\n          745.609],\n        [ 183.759,  188.886,  368.915, ...,  750.265,  771.006,\n          708.641],\n        [ 188.523,  182.887,  388.006, ...,  759.472,  700.728,\n          735.546],\n...\n        [ 187.093,  217.335,  268.847, ...,  600.192,  760.567,\n          755.355],\n        [ 165.12 ,  131.573,  447.16 , ...,  730.651,  634.832,\n          670.844],\n        [ 194.342,  155.376,  278.762, ...,  557.058,  670.72 ,\n          756.81 ]],\n\n       [[ 227.482,  215.861,  369.136, ...,  876.322,  807.167,\n          691.492],\n        [ 204.28 ,  253.91 ,  468.703, ...,  851.316,  673.234,\n          629.031],\n        [ 158.707,  185.01 ,  421.974, ..., 1219.31 , 1041.13 ,\n          994.95 ],\n        ...,\n        [ 181.02 ,  216.746,  403.83 , ...,  735.791,  757.416,\n          787.206],\n        [ 204.988,  169.965,  321.8  , ...,  875.623,  693.14 ,\n          777.141],\n        [ 215.283,  205.664,  494.475, ...,  677.682,  740.974,\n         1001.79 ]]])llik(chain, draw, llik_dim_0)float64-8.981 -8.16 ... -9.587 -87.32array([[[  -8.98149,   -8.16041, -121.834  , ...,  -26.6511 ,\n          -15.333  ,   -5.03173],\n        [  -3.3298 ,   -6.74602, -120.175  , ...,  -18.3798 ,\n          -11.9159 ,   -4.56985],\n        [  -4.73572,  -11.8407 , -113.21   , ...,  -18.8408 ,\n          -15.6229 ,   -4.65854],\n        ...,\n        [  -3.75606,  -57.2464 , -186.1    , ...,   -4.7875 ,\n           -3.85341,  -44.6675 ],\n        [ -36.9401 ,  -34.6153 , -209.275  , ...,  -58.3687 ,\n         -172.836  ,  -17.7702 ],\n        [ -12.8355 ,   -2.24944,  -85.4265 , ...,  -17.943  ,\n           -5.74474,   -3.91947]],\n\n       [[  -7.66673,   -5.33243,  -94.05   , ...,  -33.5105 ,\n           -7.87838,  -10.2459 ],\n        [ -13.3995 ,   -6.43127,  -83.7495 , ...,  -37.2591 ,\n          -17.8842 ,  -10.7848 ],\n        [  -7.4818 ,   -3.62121,  -98.3658 , ...,  -24.9407 ,\n           -3.5948 ,   -6.86394],\n...\n        [  -3.96411,  -65.3276 ,   -6.23833, ..., -161.946  ,\n          -16.6114 ,   -3.86945],\n        [ -42.2559 ,  -75.7815 , -300.021  , ...,  -44.6827 ,\n          -16.5108 ,  -33.8265 ],\n        [  -3.23865,  -16.1718 ,   -4.7924 , ..., -249.053  ,\n           -3.92796,   -8.93514]],\n\n       [[ -18.6896 ,  -57.0662 , -102.733  , ...,   -4.63053,\n          -33.0595 ,  -18.4545 ],\n        [  -4.71529, -167.057  , -299.696  , ...,   -5.00925,\n           -3.97491,  -59.8419 ],\n        [ -75.8    ,   -3.83045, -201.039  , ..., -167.683  ,\n         -186.641  ,  -81.0174 ],\n        ...,\n        [ -12.046  ,  -44.6544 , -129.648  , ...,  -32.7041 ,\n           -9.54657,   -3.67239],\n        [  -2.96314,   -4.2949 ,  -27.2165 , ...,   -4.11057,\n           -3.64532,   -3.89527],\n        [  -8.2396 ,  -22.3636 , -384.246  , ...,  -74.6057 ,\n           -9.58703,  -87.318  ]]])Indexes: (6)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))volume_ls_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23],\n      dtype='int64', name='volume_ls_dim_0'))tubePandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23],\n      dtype='int64', name='tube'))obsPandasIndexPandasIndex(Index([1, 4, 9, 11, 15, 19, 21, 22], dtype='int64', name='obs'))llik_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='llik_dim_0'))Attributes: (4)created_at :2024-04-24T15:23:11.889104arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats_prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    lp               (chain, draw) float64 32kB 72.09 71.09 ... 23.28 14.77\n    acceptance_rate  (chain, draw) float64 32kB 0.9856 0.9735 ... 0.9808 0.99\n    step_size        (chain, draw) float64 32kB 0.03423 0.03423 ... 0.04274\n    tree_depth       (chain, draw) int64 32kB 4 4 4 4 4 4 4 4 ... 7 6 7 7 6 6 7\n    n_steps          (chain, draw) int64 32kB 15 31 15 15 15 ... 127 63 63 127\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB -58.38 -61.21 ... -14.09 -3.941\nAttributes:\n    created_at:                 2024-04-24T15:23:11.891681\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)lp(chain, draw)float6472.09 71.09 72.54 ... 23.28 14.77array([[72.0913, 71.0934, 72.5368, ..., 22.7741, 19.717 , 31.5704],\n       [78.6388, 67.3879, 66.6947, ..., 27.7493, 23.3811, 21.9996],\n       [32.2751, 29.5342, 31.5931, ..., 21.5172, 20.5746, 25.7496],\n       [19.4916, 19.317 , 10.9252, ..., 36.2183, 23.2776, 14.7731]])acceptance_rate(chain, draw)float640.9856 0.9735 ... 0.9808 0.99array([[0.985585, 0.973493, 0.926252, ..., 0.999961, 0.998114, 0.999975],\n       [0.987531, 0.826221, 0.990511, ..., 0.999879, 0.999778, 0.999777],\n       [0.989942, 0.996745, 0.999915, ..., 0.997515, 0.999232, 0.999938],\n       [0.999604, 0.99648 , 0.998834, ..., 0.999231, 0.980821, 0.99004 ]])step_size(chain, draw)float640.03423 0.03423 ... 0.04274 0.04274array([[0.0342255, 0.0342255, 0.0342255, ..., 0.0342255, 0.0342255,\n        0.0342255],\n       [0.0211665, 0.0211665, 0.0211665, ..., 0.0211665, 0.0211665,\n        0.0211665],\n       [0.0399343, 0.0399343, 0.0399343, ..., 0.0399343, 0.0399343,\n        0.0399343],\n       [0.0427424, 0.0427424, 0.0427424, ..., 0.0427424, 0.0427424,\n        0.0427424]])tree_depth(chain, draw)int644 4 4 4 4 4 4 4 ... 7 7 6 7 7 6 6 7array([[4, 4, 4, ..., 7, 7, 7],\n       [5, 5, 5, ..., 8, 8, 8],\n       [6, 7, 6, ..., 7, 7, 7],\n       [7, 7, 7, ..., 6, 6, 7]])n_steps(chain, draw)int6415 31 15 15 15 ... 127 63 63 127array([[ 15,  31,  15, ..., 127, 255, 127],\n       [ 31,  31,  31, ..., 255, 255, 255],\n       [ 63, 255,  63, ..., 127, 127, 127],\n       [127, 127, 127, ...,  63,  63, 127]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64-58.38 -61.21 ... -14.09 -3.941array([[-5.83823e+01, -6.12113e+01, -6.32211e+01, ..., -1.00715e+01,\n         3.57843e-02, -1.26730e+01],\n       [-7.05147e+01, -5.28132e+01, -5.87134e+01, ..., -1.31417e+01,\n        -1.17634e+01, -5.75708e+00],\n       [-2.44030e+01, -1.81596e+01, -2.17799e+01, ..., -9.25289e+00,\n        -6.95541e+00, -9.89278e+00],\n       [-5.78982e+00, -5.81860e+00, -6.76507e-01, ..., -1.92249e+01,\n        -1.40883e+01, -3.94102e+00]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-24T15:23:11.891681arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 896B\nDimensions:               (N_dim_0: 1, N_cal_dim_0: 1, tube: 24, obs: 8,\n                           measurement_ix_dim_0: 8, cal_error_dim_0: 1,\n                           likelihood_dim_0: 1)\nCoordinates:\n  * N_dim_0               (N_dim_0) int64 8B 0\n  * N_cal_dim_0           (N_cal_dim_0) int64 8B 0\n  * tube                  (tube) int64 192B 0 1 2 3 4 5 6 ... 18 19 20 21 22 23\n  * obs                   (obs) int64 64B 1 4 9 11 15 19 21 22\n  * measurement_ix_dim_0  (measurement_ix_dim_0) int64 64B 0 1 2 3 4 5 6 7\n  * cal_error_dim_0       (cal_error_dim_0) int64 8B 0\n  * likelihood_dim_0      (likelihood_dim_0) int64 8B 0\nData variables:\n    N                     (N_dim_0) int64 8B 24\n    N_cal                 (N_cal_dim_0) int64 8B 8\n    target_volume         (tube) int64 192B 200 200 200 200 ... 800 800 800 800\n    y                     (obs) float64 64B 199.1 176.3 281.9 ... 693.9 783.4\n    measurement_ix        (measurement_ix_dim_0) int64 64B 2 5 10 12 16 20 22 23\n    cal_error             (cal_error_dim_0) float64 8B 0.02\n    likelihood            (likelihood_dim_0) int64 8B 1\n    true_volume           (tube) float64 192B 152.6 199.7 161.3 ... 770.5 598.6\nAttributes:\n    created_at:                 2024-04-24T15:23:11.893640\n    arviz_version:              0.17.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.1xarray.DatasetDimensions:N_dim_0: 1N_cal_dim_0: 1tube: 24obs: 8measurement_ix_dim_0: 8cal_error_dim_0: 1likelihood_dim_0: 1Coordinates: (7)N_dim_0(N_dim_0)int640array([0])N_cal_dim_0(N_cal_dim_0)int640array([0])tube(tube)int640 1 2 3 4 5 6 ... 18 19 20 21 22 23array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23])obs(obs)int641 4 9 11 15 19 21 22array([ 1,  4,  9, 11, 15, 19, 21, 22])measurement_ix_dim_0(measurement_ix_dim_0)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])cal_error_dim_0(cal_error_dim_0)int640array([0])likelihood_dim_0(likelihood_dim_0)int640array([0])Data variables: (8)N(N_dim_0)int6424array([24])N_cal(N_cal_dim_0)int648array([8])target_volume(tube)int64200 200 200 200 ... 800 800 800 800array([200, 200, 200, 200, 200, 200, 200, 200, 400, 400, 400, 400, 400,\n       400, 400, 400, 800, 800, 800, 800, 800, 800, 800, 800])y(obs)float64199.1 176.3 281.9 ... 693.9 783.4array([199.07727295, 176.25632772, 281.87757567, 387.16351159,\n       362.14946827, 853.23878527, 693.91934176, 783.39963414])measurement_ix(measurement_ix_dim_0)int642 5 10 12 16 20 22 23array([ 2,  5, 10, 12, 16, 20, 22, 23])cal_error(cal_error_dim_0)float640.02array([0.02])likelihood(likelihood_dim_0)int641array([1])true_volume(tube)float64152.6 199.7 161.3 ... 770.5 598.6array([152.64294349, 199.70810807, 161.32449302, 171.49715397,\n       174.67894069, 163.43175945, 153.50063823, 187.79919405,\n       364.94147086, 289.55488638, 445.13256694, 387.79655766,\n       326.2592979 , 385.23402356, 335.94110379, 349.87019831,\n       761.78380169, 620.86368119, 745.73037525, 809.71007835,\n       803.52489045, 683.21425317, 770.52360505, 598.61585552])Indexes: (7)N_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='N_dim_0'))N_cal_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='N_cal_dim_0'))tubePandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23],\n      dtype='int64', name='tube'))obsPandasIndexPandasIndex(Index([1, 4, 9, 11, 15, 19, 21, 22], dtype='int64', name='obs'))measurement_ix_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='measurement_ix_dim_0'))cal_error_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='cal_error_dim_0'))likelihood_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='likelihood_dim_0'))Attributes: (4)created_at :2024-04-24T15:23:11.893640arviz_version :0.17.0inference_library :cmdstanpyinference_library_version :1.2.1\n                      \n                  \n            \n            \n              \n            \n            \n\n\nNext we look at the summaries of both the posterior and prior.\n\nfor group_name in [\"prior\", \"posterior\"]:\n    group = idata.get(group_name)\n    group_summary = az.summary(\n        group,\n        var_names=[\n            \"volume_noise\", \"bias_factor\", \"volume_noise_s\", \"bias_factor_l\"\n        ]\n    )\n    display(group_summary)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nvolume_noise\n0.066\n0.034\n0.016\n0.128\n0.002\n0.001\n224.0\n380.0\n1.01\n\n\nbias_factor\n0.994\n0.143\n0.731\n1.252\n0.015\n0.010\n80.0\n120.0\n1.06\n\n\nvolume_noise_s\n0.108\n0.055\n0.026\n0.210\n0.003\n0.002\n224.0\n380.0\n1.01\n\n\nbias_factor_l\n-0.016\n0.141\n-0.286\n0.242\n0.015\n0.011\n80.0\n120.0\n1.06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nvolume_noise\n0.113\n0.026\n0.068\n0.160\n0.001\n0.001\n1303.0\n1943.0\n1.0\n\n\nbias_factor\n0.887\n0.055\n0.781\n0.986\n0.001\n0.001\n2493.0\n2380.0\n1.0\n\n\nvolume_noise_s\n0.185\n0.043\n0.111\n0.261\n0.001\n0.001\n1303.0\n1943.0\n1.0\n\n\nbias_factor_l\n-0.122\n0.061\n-0.229\n0.002\n0.001\n0.001\n2493.0\n2380.0\n1.0\n\n\n\n\n\n\n\n\n\n\nInvestigating the results\nThis plot compares the measurements with the observations.\n\naz.plot_lm(\n    y=idata.observed_data[\"y\"],\n    x=idata.observed_data[\"tube\"].sel(tube=MEASUREMENT_IX + 1),\n    y_hat=idata.posterior_predictive[\"y_rep\"]\n)\nax = plt.gca()\nax.semilogy()\n\n\n\n\n\n\n\n\nThis plot compares the volume_noise and bias_factor samples with the true values that we used to simulate the data.\n\naz.plot_posterior(\n  idata.prior,\n  var_names=[\"volume_noise\", \"bias_factor\"],\n  kind=\"hist\",\n  hdi_prob=\"hide\",\n  point_estimate=None,\n  figsize=[12, 4]\n)\nf = plt.gcf()\naxes = f.axes\naz.plot_posterior(\n  idata.posterior,\n  var_names=[\"volume_noise\", \"bias_factor\"],\n  kind=\"hist\",\n  hdi_prob=\"hide\",\n  point_estimate=None,\n  figsize=[12, 4],\n  ax=axes,\n  color=\"tab:orange\"\n)\nfor ax, truth in zip(f.axes, [NOISE, BIAS_FACTOR]):\n    ax.axvline(truth, color=\"red\")\n\n\n\n\n\n\n\n\nThis plot shows the samples for all the tubes’ volumes, including those that weren’t measured, alongside the true volumes.\n\naz.plot_lm(\n    x=idata.observed_data[\"tube\"],\n    y=idata.observed_data[\"true_volume\"],\n    y_hat=idata.posterior[\"volume\"],\n    grid=False,\n    y_kwargs={\"label\": \"true volume\"},\n    figsize=[10, 5],\n    legend=False,\n)\nax = plt.gca()\nfor i in MEASUREMENT_IX:\n    ax.text(i+0.1, volumes[i], \"obs\", zorder=1000)\nax.set(xlabel=\"tube\", ylabel=\"volume ($\\\\mu$l)\");\nax.semilogy()\n\n\n\n\n\n\n\n\nSo, what is the probability that Teddy put less than 350 \\(\\mu\\)l of label into tube 10, even though the target amount was 400\\(\\mu\\)l?\n\naz.plot_posterior(\n  idata.prior,\n  var_names=[\"volume\"],\n  coords={\"tube\": [10]},\n  kind=\"hist\",\n  hdi_prob=\"hide\",\n  point_estimate=None,\n  ref_val=350,\n  figsize=[12, 4],\n  bins=np.linspace(250, 600, 30),\n)\n\n\n\n\n\n\n\n\nPhew, only about 13%, that’s probably fine right?",
    "crumbs": [
      "Course materials",
      "Regression models for describing measurements"
    ]
  }
]